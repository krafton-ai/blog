<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://krafton-ai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://krafton-ai.github.io/blog/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-07-28T22:55:06+08:00</updated><id>https://krafton-ai.github.io/blog/feed.xml</id><title type="html">Krafton AI</title><subtitle>Home to the Krafton AI Blog
</subtitle><entry><title type="html">Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning</title><link href="https://krafton-ai.github.io/blog/llm_post_training/" rel="alternate" type="text/html" title="Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning" /><published>2025-07-28T00:00:00+08:00</published><updated>2025-07-28T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/llm_post_training</id><content type="html" xml:base="https://krafton-ai.github.io/blog/llm_post_training/"><![CDATA[<h1 id="pushing-the-limits-of-mathematical-reasoning-via-off-policy-grpo-based-continual-post-training">Pushing the Limits of Mathematical Reasoning via Off-Policy GRPO-based Continual Post-Training</h1>
<!-- Affliation: KRAFTON & SKT -->
<p>우리는 여러 추론형 Large Language Models (LLMs) 들의 수학 추론 성능을 더 끌어올리 위해 활용할 수 있는 continual post-training 방법을 제안했습니다.
기존에 많이 활용되는 on-policy GRPO는 매 step마다 model에서 rollout을 통해 sample을 뽑아야 하기 때문에 rollout 과정에서 많은 시간이 소요될 뿐 아니라, 학습 초기의 base LLM 성능이 좋지 않을 경우 높은 reward를 갖는 좋은 sample 자체를 거의 얻을 수 없다는 문제가 있습니다.
이러한 단점을 해결하기 위해, 미리 준비해둔 teacher의 reasoning trajectory만을 활용하여 off-policy GRPO를 진행합니다. 
추가적으로, 기존 GRPO는 group 내 모든 sample이 positive일 경우 advantage가 0이 되어 학습이 이루어지지 않는데, 여기선 teacher가 생성한 trajectory만을 다룸으로 모두 positive이더라도 supervision을 받을 수 있도록 GRPO의 advantage에 bias term을 추가하는 방식을 제안합니다.
최종적으로, Figure 1 과 같이 수학에 더 특화된 성능을 얻은 LLM들을 얻을 수 있었습니다.
또한, 이 학습 방법으로 수학이 아닌 다른 task에서는 기존 모델과 유사한 성능을 유지할 수 있음을 확인했습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/radar_charts.png" style="display: inline-block; width: 100%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 1: Performance Comparision between base LLMs and base LLMs with our method.</figcaption>
</figure>

<h2 id="why-do-we-focus-on-post-training-for-reasoning">Why do we focus on post-training for reasoning?</h2>
<p>Large text corpus로 학습하는 LLM의 사전학습(pre‑training)은 언어의 통계적 패턴과 구조를 이해하는 데 큰 역할을 합니다.
하지만 단계를 거쳐야 하는 복잡한 논리 추론, 수학적 문제 해결, 또는 코드 작성과 같은 작업에서는 충분하지 않습니다.
예컨대 Chain-of-Thought (CoT)를 쓰면 어느 정도 추론 과정을 모방하게 할 수 있지만, 이는 모델이 일관된 사고 흐름을 자기 것으로 내재화한 것이 아니라 성능의 한계가 존재합니다.</p>

<p>이런 한계를 극복하기 위한 post-training 방법 중 하나로 Reinforcement Learning with Verifiable Rewards (RLVR) 이 있습니다.
이 방식의 post-training이 LLM의 추론 강화에 효과적이라는 사실은 DeepSeek-R1 이나 OpenAI의 o1/o3 모델과 같은 모델들이 RLVR 방식을 통해 실질적인 추론 성능향상을 보이면서 더 각광을 받게 되었습니다.
RLVR은 LLM이 생성한 전체 추론 과정인 chain-of-thought(CoT) 형태의 rollout trajectory가 검증 가능해야 합니다.
예를 들어, 수학 문제의 정답이 있어서 최종 정답이 비교가 되거나, 각 단계에 대한 평가가 가능해, 생성된 rollout trajectory 전체에 대해 보상(reward)을 줄 수 있는 구조입니다.
이런 보상을 바탕으로, RL 알고리즘(e.g., PPO, GRPO)을 통해, LLM은 검증된 추론 과정을 더 많이 생성하도록 학습되게 됩니다.</p>

<p>대표적인 RLVR 방식의 학습 알고리즘으로는 Group Relative Policy Optimization (GRPO) 방식 <d-cite key="guo2025deepseekR1"></d-cite>이 있습니다.
이 방식은 동일 질문에 대해 샘플링한 여러 rollout trajectory들의 그룹의 평균 보상을 기준으로 삼아 각 응답의 상대적 advantage를 계산합니다.
이의 objective $\mathcal{L}$을 수식으로 표현하면 아래와 같습니다:</p>

\[\mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{old}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>여기서, $\pi^{i,t} = \pi(o_{i,t}\mid q, o_{i,&lt;t})$이며, $G$는 한 그룹 내 응답 개수를 의미합니다. 또한, $\pi_\text{ref}$, $\pi_{\theta}$, $\pi_\text{old}$는 각각 기준(reference) LLM, 학습 중인 LLM, 샘플링에 사용되는 LLM을 나타냅니다. $q$와 ${o_i}_{i=1}^{G}$는 학습에 사용되는 질문과 생성된 rollout trajectory 세트를 나타냅니다.
각 응답의 상대적 advantage인 $\hat{A}_i$는 다음과 같이 계산됩니다.</p>

\[\hat{A}_i = \frac{r_i - \text{mean}(r_i)}{\text{std}(r_i)}\]

<p>이렇게 계산된 trajectory level 의 advantage는 응답의 각 토큰 수준에 적용되어 최종적으로 $\hat{A}_{i,t}$로 사용됩니다.</p>

<h2 id="off-policy-reinforcement-learning-with-verifiable-reward-off-policy-rlvr">Off-policy Reinforcement Learning with Verifiable Reward (Off-policy RLVR)</h2>
<h3 id="why-do-we-focus-on-off-policy-rlvr-eg-grpo">Why do we focus on Off-policy RLVR (e.g., GRPO)?</h3>
<p>앞서 설명한 RLVR 의 경우는 on-policy 로 advantage를 업데이트 하는 방식입니다.
On-policy 의 경우, 학습하는 모델이 뽑아낸 roll-out trajectory 에 대해 reward 를 계산해 이를 통해 LLM을 업데이트 한다는 것을 의미합니다.</p>

<p>하지만, on-policy로 학습하는 것에는 두가지 단점이 존재합니다.
첫번째 단점은 학습 속도가 매우 느리다는 것입니다. <d-cite key="liang2025squeeze"></d-cite>
이 이유는 매 스텝 loss 를 계산할 때 roll-out 을 각 문제마다 $G$ 개수만큼 해야하기 때문입니다.
또한, on-policy 는 기존 base LLM 성능에 한계가 제한됩니다. <d-cite key="luffy"></d-cite>
기존 base LLM 의 성능이 낮으면 정답 trajectory 를 뽑을 확률이 낮아져 성능 향상의 bottleneck 이 생길 수 있기 때문입니다.</p>

<p>On-policy 의 문제들을 해결하기 위해, 저희는 정답 label 을 뽑기 위해 쓴 teacher LLM의 roll-out trajectory 를 활용하는 off-policy RLVR 방식을 도입했습니다.
저희가 이번 프로젝트에 활용한 데이터는 OpenThought3 <d-cite key="openthoughts"></d-cite> 데이터셋입니다.
OpenThought3는 Math, Code, Science 등에 관련된 문제들에 대해 teacher model 인 QwQ-32B 모델을 이용해 sampling 을 16번씩 진행해 만든 데이터입니다.
이 중, 저희는 정답이 확실한 Math 에 대해서만 먼저 filtering 하여 준비했습니다.
그 뒤, 각 문제마다 Majority Voting <d-cite key="wangself"></d-cite> 을 이용해 정답으로 간주될 수 있는 답들을 찾고, 이를 바탕으로 각 trajectory 의 reward 를 계산하였습니다.</p>

<p>이 때, Teacher model 을 $\theta_\text{teacher}$ 라고 하고, teacher가 정답 label을 뽑기 위해 roll-out 한 문제들의 random variable을 $Q$ 라고 할 때, 우리는 off-policy GRPO를 아래와 같이 수식을 쓸 수 있습니다:</p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>이 때, practical 하게 teacher 의 probability 를 모르기 때문에, $\pi_{\theta_\text{teacher}}^{i,t}=1$ 로 가정하였습니다.
그러면 항상 $\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}} \le 1$ 이기 때문에, $\text{clip}$ 은 $\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)$ 로 표현될 수 있습니다.
최종적으로, practical하게 off-policy 에 해당하는 GRPO 수식은 아래와 같이 근사될 수 있습니다. <d-cite key="luffy"></d-cite></p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\pi_{\theta}^{i,t}\hat{A}_{i,t},\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<h3 id="off-policy-grpo-vs-supervised-fine-tuning-sft">Off-policy GRPO vs. Supervised Fine-tuning (SFT)</h3>

<p>제일 간단하게 off-policy 로 준비된 teacher sample 로 학습한다는 것은 SFT를 하는 것으로 생각할 수 있습니다.
그러면 SFT 와 Off-policy GRPO 는 무슨 차이가 있을까요?
아래의 표현된 SFT 수식과 off-policy GRPO 의 수식을 비교하면 그 차이를 알 수 있습니다.</p>

\[\mathcal{L}_{SFT}(\theta) = \mathbb{E}_{q \sim P(Q),\,o \sim \pi_{\theta_\text{teacher}}^{(+)}(O|q)}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}\log\pi_{\theta}(o_{t}|q,o_{\lt t}) - \beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right]\]

<p>여기서, $\pi_{\theta_\text{teacher}}^{(+)}(O|q)$는 teacher가 생성한 샘플 중에서 positive reward를 갖는 샘플만 선택했음을 나타냅니다.
이를 통해 모델은 오로지 positive 샘플의 분포를 따라가도록 지도학습을 수행하게 됩니다.</p>

<p>Practical하게 근사한 off-policy GRPO 수식과 비교했을 때, 우리의 Off-policy GRPO 수식은 negative 에 해당하는 sample 또한 반영된다는 점입니다.
Off-policy GRPO 에서는 positive 와는 계속 가까워지게 학습을 하며, 반대로 negative 와는 멀어지게 학습을 하는 구조가 됩니다.
이로 인해 논리적으로 틀려 가지 않아야 할 rollout trajectory 에 멀어지게 되어, 더 추론 능력이 강화될 수 있습니다.
이 사실을 아래 실험을 통해 확인해 보았습니다.</p>

<h3 id="lets-try-out-our-experiment">Let’s try out our experiment</h3>

<p>우리는 off-policy GRPO 방식이 단순히 positive 샘플만을 학습하는 supervised fine-tuning (SFT) 방식보다 추론 능력 강화에 더 효과적인지 확인하기 위해 실험을 수행했습니다. 
실험에 사용된 모델은 OpenThinker3-7B이며, 데이터는 OpenThought3의 Math 문제 중 약 2,000개를 sampling하여 구성했습니다. 
실험 환경은 VERL 프레임워크를 기반으로 구축되었고, 학습 과정에서 learning rate는 1e-7, batch size는 32, epoch는 3, KL coefficient는 0.1로 설정했습니다. 이때 각 문제당 8개의 샘플을 사용하여 학습을 진행했습니다.</p>

<p>성능 평가는 AIME24를 validation set으로 사용하여 각 epoch마다 성능을 측정했고, 3 epoch 중 가장 우수한 성능을 보인 checkpoint를 선정하여 최종적으로 AIME25-1과 AIME25-2에서 성능을 비교했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>SFT</td>
      <td><strong>55.83</strong></td>
      <td>57.4</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td><strong>63.02</strong></td>
    </tr>
  </tbody>
</table>

<p>Off-policy GRPO 방식이 SFT보다 종합적으로 더 뛰어난 성능을 보인 이유는 negative sample들로부터 모델이 적극적으로 멀어지도록 학습을 유도했기 때문입니다. 
기존의 데이터는 모두 positive sample로 고려되고 있지만, 본 실험에서는 majority voting과 Off-policy GRPO를 통해 보다 정확한 정보를 확보하고 이를 바탕으로 negative sample로부터 벗어나도록 유도하여, 더욱 세밀한 지도학습이 가능했습니다.</p>

<p>본 실험을 통해 Off-policy GRPO 방식이 기존 SOTA LLM의 논리적 추론 능력을 한 단계 더 향상시키는 데 매우 효과적임을 입증했습니다. 이 방식은 향후 다양한 추론 도메인에서도 충분히 적용 가능할 것으로 기대됩니다.</p>

<h2 id="proposed-loss-for-rlvr-challenges--solutions">Proposed Loss for RLVR: Challenges &amp; Solutions</h2>
<h3 id="challenges-considering-all-positive-reasoning-trace">Challenges: Considering all positive reasoning trace</h3>
<p>Off-policy GRPO 는 그럼 문제가 없을까요?
아닙니다. 이유는 roll-out trajectory 가 all positive인 sample의 Advantage 값에 있습니다.
실제로 어떤 quesiton에 대해서는 전부 다 positive 인 sample 이 존재 할 수 있습니다.
이런 데이터의 경우 advantage 값을 계산하게 되면 mean reward 가 1 이 되어 0이 되고, 그러면 그 sample 들에 대해서는 update 가 되지 않습니다.</p>

<p>On-policy 의 경우에서는 이런 all positive sample 들이 부분이 문제가 안됩니다.
왜냐하면 이미 모델이 잘하는 경우라 update 를 하지 않아도 되기 때문입니다.</p>

<p>하지만, 반대로 Off-policy 의 경우에는 all positive sample 들을 학습하지 않는 것이 도움이 되지 않는 경우가 있습니다.
Off-policy 로 준비한 sample 이 teacher sample 인 저희의 case 의 경우를 생각해 보겠습니다.
이 경우, teacher sample 은 전부 다 positive 여서 reward 가 1이지만, 실제 그 문제에 대해 저희가 학습할 target 모델은 틀린 sample 을 생성할 가능성이 높을 수 있습니다.
하지만 현재 Objective 로는 이 부분을 고려하지 못하고, 결국 배워야 할 rollout trajectory 에 대해 학습하지 못함을 의미합니다.</p>

<p>저희가 준비한 데이터에서 이 문제가 있음을 아래 그림을 통해 확인이 되었습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/plot_ot3_non_all_positive_pie.png" style="display: inline-block; width: 40%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 2. Statistics that although teacher can generate all positive answers, but base model cannot perfectly answer. </figcaption>
</figure>

<h3 id="proposed-method">Proposed method</h3>
<p>위 문제를 해결하기 위해, 저희는 all positive roll-out trajectory 인 경우에는 advantage 에 bias term 인 $b$를 추가하는 simple 한 변형 loss term 을 제안하였습니다.
제안 방식을 통해 all positive 인 sample 이 나올 때도 그 sample 들에 대해 update 가 되도록 해주어서, all positive reasoning trace를 반영하지 못하는 문제를 해결합니다.
자세한 수식은 아래와 같습니다.</p>

\[\mathcal{L}_{ours}(\theta) =
\begin{cases}
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t}(\hat{A}_{i,t} + b), \max(\pi_{\theta}^{i,t}, 1-\epsilon)(\hat{A}_{i,t} + b)\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{if all } r_{i} &gt; 0 \\
\\
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t} \hat{A}_{i,t}, \max(\pi_{\theta}^{i,t}, 1-\epsilon)\hat{A}_{i,t}\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{otherwise}
\end{cases}\]

<p>저희는 $b=0.5$ 로 setting 을 하고 실험을 진행했습니다.</p>

<h3 id="lets-try-out-our-experiment-1">Let’s try out our experiment</h3>

<p>본 실험에서는 제안한 방식이 기존 off-policy GRPO보다 성능 향상에 도움이 되는지 추가로 검증하고자 하였습니다. 
특히, On-policy GRPO와의 성능 차이도 함께 비교하기 위해 추가적으로 실험을 진행하였습니다. On-policy GRPO 방식의 경우 각 문제마다 동일하게 8개의 샘플을 직접 roll-out하여 학습을 진행했습니다.</p>

<p>실험 환경은 첫 번째 실험과 완벽히 동일하며, 오직 학습 objective만 달라졌습니다. 
동일한 모델(OpenThinker3-7B), 동일한 데이터(OpenThought3 Math 문제 2,000개), 동일한 hyperparameter(learning rate=1e-7, batch size=32, epoch=3, KL coefficient=0.1, 각 문제당 8개의 샘플)를 사용했습니다. 
또한 bias term은 0.5로 설정하여 진행했습니다.</p>

<p>성능 평가는 동일한 방식으로 AIME24를 validation set으로 사용하여 best checkpoint를 선정한 후, 최종적으로 AIME25-1과 AIME25-2를 통해 성능을 평가했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>On-policy GRPO</td>
      <td>55.42</td>
      <td>59.58</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td>63.02</td>
    </tr>
    <tr>
      <td>Off-policy GRPO (with all positive group bias)</td>
      <td><strong>56.35</strong></td>
      <td><strong>63.85</strong></td>
    </tr>
  </tbody>
</table>

<p>실험 결과, 제안한 modified off-policy GRPO 방식이 기존 off-policy GRPO 방식뿐만 아니라 On-policy GRPO 방식보다도 더 높은 성능을 기록했습니다. 
이는 앞서 언급된 바와 같이, On-policy 방식이 가진 base model 성능에 의해 제한된다는 intrinsic한 단점 때문으로 추측됩니다.</p>

<p>종합적으로, 제안한 modified loss 방식이 기존 off-policy GRPO의 한계였던 all-positive 샘플의 학습 문제를 성공적으로 극복했으며, 이를 통해 추론 성능을 더욱 강화할 수 있음을 확인했습니다. 
향후 다른 다양한 추론 도메인에서도 이 방법론이 효과적으로 활용될 가능성이 클 것으로 기대됩니다.</p>

<h2 id="generalization-across-multiple-models-and-benchmarks">Generalization Across Multiple Models and Benchmarks</h2>

<p>우리는 제안한 continual post-training 방법론이 더 광범위한 모델과 다양한 벤치마크에서도 일관된 성능 개선을 보이는지 추가적으로 확인해 보았습니다. 
이를 위해, 앞선 실험의 세팅을 유지한 상태에서 추가적으로 두 가지 모델(Openthinker2-7B, AceReason-Nemetron-1.1-7B)을 사용해 평가했습니다. 
또한 평가를 위해 선택한 벤치마크는 수학 문제 해결 능력을 평가하는 AIME25와 AMC23, 코드 생성 능력을 평가하는 LiveCodeBench, 일반적 추론 능력을 평가하는 GPQA-Diamond, 그리고 사실 검증 성능을 평가하는 IFEval입니다.
여기서 AIME25와 AMC23, LiveCodeBench, GPQA-Diamond 평가에는 <a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> framework, IFEval 평가엔 <a href="https://github.com/EleutherAI/lm-evaluation-harness">llm-evaluation-harness</a> framework을 사용하였습니다.</p>

<p>실험 결과는 아래 표와 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>AIME25</th>
      <th>AMC23</th>
      <th>LiveCodeBench</th>
      <th>GPQA-Diamond</th>
      <th>IFEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Openthinker3-7B</td>
      <td>Base</td>
      <td>57.2915</td>
      <td>92.617</td>
      <td>63.968</td>
      <td>50.947</td>
      <td>50.09</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>59.5315</td>
      <td>93.516</td>
      <td>64.995</td>
      <td>49.684</td>
      <td>51.66</td>
    </tr>
    <tr>
      <td>Openthinker2-7B</td>
      <td>Base</td>
      <td>39.792</td>
      <td>88.633</td>
      <td>56.115</td>
      <td>45.833</td>
      <td>53.3</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>40.3645</td>
      <td>87.656</td>
      <td>55.944</td>
      <td>46.843</td>
      <td>52.20</td>
    </tr>
    <tr>
      <td>AceReason-Nemetron-1.1-7B</td>
      <td>Base</td>
      <td>64.635</td>
      <td>92.93</td>
      <td>72.383</td>
      <td>52.462</td>
      <td>36.02</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>65.521</td>
      <td>93.164</td>
      <td>72.603</td>
      <td>54.356</td>
      <td>38.23</td>
    </tr>
  </tbody>
</table>

<p>실험 결과를 통해 Math 데이터로 post-training한 만큼 수학 관련 벤치마크(AIME25, AMC23)에서는 모든 모델에서 일관된 성능 향상이 관찰되었습니다. 
흥미롭게도, 다른 도메인에서의 성능 또한 기존 대비 저하되지 않고 유지되었으며, 특히 코드 생성 벤치마크인 LiveCodeBench에서는 오히려 소폭의 성능 향상이 관찰되었습니다. 
이는 continual post-training 과정에서도 catastrophic forgetting이 거의 발생하지 않았다는 것을 의미합니다.</p>

<p>추가적으로, off-policy GRPO 방식은 실제 학습 과정에서 직접적인 rollout 없이 teacher model의 trajectory를 재활용하기 때문에 매우 효율적입니다. 
이러한 효율성 덕분에 기존의 reinforcement learning 방식 대비 계산 자원을 현저히 절약하면서도, 성능은 동등하거나 더 뛰어난 결과를 달성했습니다. 
따라서 저희가 제안한 continual post-training 방법론은 성능과 효율성을 모두 갖춘 실용적인 접근법으로, 향후 다양한 도메인과 응용 분야에서도 매우 유용하게 활용될 수 있을 것으로 기대합니다.
마지막으로, 오픈 소스 커뮤니티에 기여하고 연구의 재현성과 확장 가능성을 높이기 위해 <a href="https://huggingface.co/KRAFTON">huggingface</a>에 평가한 모델들을 모두 공개하였습니다.</p>]]></content><author><name>KRAFTON</name></author><summary type="html"><![CDATA[이 포스팅에서는 거대 언어 모델(LLM)의 추론 능력을 지속적인 추가 학습(Continual Post-training)을 통해 향상시키는 새로운 접근법을 소개합니다. 기존의 사전 학습(Pre-training)은 광범위한 언어적 지식을 제공하지만, 수학이나 코드 생성과 같은 복잡한 추론 작업에서는 여전히 한계를 드러냅니다. 최근 연구에서는 강화학습 기반의 검증 가능한 보상 기법(RLVR)이 이러한 한계를 어느 정도 해소할 수 있다고 밝혔으나, 현재까지 제안된 방법들은 느리고 제한적인 온-폴리시(On-policy) 학습에 의존하고 있습니다. 우리는 이 문제를 해결하기 위해 교사 모델이 생성한 우수한 추론 경로(Trajectory)만을 활용하는 오프-폴리시(Off-policy) 학습 방식을 제안하며, 특히 모든 생성된 결과가 긍정적인 경우에도 고품질의 추론 과정을 효과적으로 학습할 수 있도록 GRPO 알고리즘의 새로운 변형 방식을 도입했습니다. 수학적 추론을 대상으로 한 실험 결과, 제안한 방법이 일관된 성능 향상을 보임을 확인하였습니다.]]></summary></entry><entry><title type="html">[EN] Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning</title><link href="https://krafton-ai.github.io/blog/llm_post_training_en/" rel="alternate" type="text/html" title="[EN] Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning" /><published>2025-07-28T00:00:00+08:00</published><updated>2025-07-28T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/llm_post_training_en</id><content type="html" xml:base="https://krafton-ai.github.io/blog/llm_post_training_en/"><![CDATA[<h1 id="boosting-math-reasoning-in-open-source-llms-via-continual-post-training">Boosting Math Reasoning in Open-Source LLMs via Continual Post-Training</h1>
<!-- Affliation: KRAFTON & SKT -->
<p>We propose a continual post-training method that further enhances the mathematical reasoning performance of various Large Language Models (LLMs). 
Although on-policy GRPO is a commonly used for post-training, it suffers from long training times and is constrained by the capabilities of the base LLM. 
To address these issues, we use an off-policy GRPO method, which learns from a pre-existing dataset of high-quality solutions generated by a more capable teacher model.
Additionally, we introduce a bias term in the advantage function to ensure that all positive trajectories are effectively learned. 
As a result, we developed LLMs with specialized performance in mathematics, as illustrated in Figure 1. 
We also confirmed that this training method maintains the performance of the original model on tasks other than mathematics.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/radar_charts.png" style="display: inline-block; width: 100%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 1: Performance Comparision between base LLMs and base LLMs with our method.</figcaption>
</figure>

<h2 id="why-do-we-focus-on-post-training-for-reasoning">Why do we focus on post-training for reasoning?</h2>
<p>The pre-training of LLMs on large text corpora plays a crucial role in capturing statistical patterns and language structures. 
However, this is often insufficient for tasks that require complex, multi-step logical reasoning, mathematical problem-solving, or code generation. 
For instance, while using Chain-of-Thought (CoT) can mimic the reasoning process to some extent, its performance is limited because the model has not fully internalized a coherent flow of reasoning.</p>

<p>Reinforcement Learning with Verifiable Rewards (RLVR) is designed to overcome these limitations. 
The effectiveness of this post-training approach in enhancing the reasoning capabilities of LLMs has gained prominence, with models like DeepSeek-R1 and OpenAI’s o1/o3 models demonstrating substantial improvements in reasoning performance via RLVR. 
RLVR requires the entire reasoning process generated by the LLM—a rollout trajectory in the form of Chain-of-Thought (CoT)—to be verifiable. 
For example, in a math problem with a known answer, the final solution can be directly compared, or each step can be evaluated, enabling reward assignment to the entire generated trajectory. 
Based on such rewards, RL algorithms (e.g., PPO, GRPO) are used to train the LLM to generate correct reasoning processes.</p>

<p>Group Relative Policy Optimization (GRPO) <d-cite key="guo2025deepseekR1"></d-cite>, one of the most commonly used algorithms in RLVR, calculates each response’s relative advantage based on the average reward of a group of sampled rollout trajectories for the same question. 
The objective function $\mathcal{L}$ can be expressed as follows:</p>

\[\mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{old}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>where $\pi^{i,t} = \pi(o_{i,t}\mid q, o_{i,&lt;t})$, and $G$ represents the number of responses in a group. Also, $\pi_\text{ref}$, $\pi_{\theta}$, and $\pi_\text{old}$ denote the reference LLM, the LLM being trained, and the LLM from the previous step used for sampling, respectively. $q$ and ${o_i}_{i=1}^{G}$ denote the set of questions and the generated rollout trajectories used for training. 
The relative advantage of each response $\hat{A}_i$ is calculated as follows:</p>

\[\hat{A}_i = \frac{r_i - \text{mean}(r_i)}{\text{std}(r_i)}\]

<p>This trajectory-level advantage is then applied to each token level of the response, ultimately being used as $\hat{A}_{i,t}$.</p>

<h2 id="off-policy-reinforcement-learning-with-verifiable-reward-off-policy-rlvr">Off-policy Reinforcement Learning with Verifiable Reward (Off-policy RLVR)</h2>
<h3 id="why-do-we-focus-on-off-policy-rlvr-eg-grpo">Why do we focus on Off-policy RLVR (e.g., GRPO)?</h3>
<p>The RLVR method described earlier computes the advantage in an on-policy manner.
This means that the LLM is updated based on the rewards of the rollout trajectories generated by itself.</p>

<p>However, on-policy training has two main drawbacks. 
The first is its slow training speed. <d-cite key="liang2025squeeze"></d-cite> 
This is due to the need for the generation of $G$ rollout trajectories per question at each step of loss calculation.
Second, the performance of on-policy methods is limited by the capabilities of the base LLM. <d-cite key="luffy"></d-cite> 
With a less-capable base LLM, we get a relatively small probability of generating a correct trajectory, which can lead to a bottleneck for performance improvement.</p>

<p>To overcome these drawbacks of on-policy methods, we turn our attention to off-policy RLVR methods that utilize pre-existing rollout trajectories of teacher LLMs.
In this project, we use the OpenThoughts3 <d-cite key="openthoughts"></d-cite> dataset.
The dataset consists of 16 roll-out trajectories of the teacher model, QwQ-32B, on problems in math, code, and science.
From the dataset, we only utilize math problems.
We then apply Majority Voting <d-cite key="wangself"></d-cite> to identify potentially correct answers and assign rewards to each trajectory accordingly.</p>

<p>Denoting the teacher model $\theta_\text{teacher}$ and the random variable for the problems that the teacher has seen as $Q$, we can write the off-policy GRPO equation as follows:</p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>Since we do not know the likelihood of the trajectory under the teacher model in practice, we assume $\pi_{\theta_\text{teacher}}^{i,t} = 1$.
This means that $\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}} \le 1$ is always true, $\text{clip}$ can be expressed as $\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)$.
Therefore, the off-policy GRPO objective can be approximated as follows: <d-cite key="luffy"></d-cite></p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\pi_{\theta}^{i,t}\hat{A}_{i,t},\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<h3 id="off-policy-grpo-vs-supervised-fine-tuning-sft">Off-policy GRPO vs. Supervised Fine-tuning (SFT)</h3>

<p>The simplest way to train a model with off-policy data is SFT.
So, what distinguishes SFT from off-policy GRPO? 
The difference becomes clear by comparing the SFT loss equation below with that of off-policy GRPO.</p>

\[\mathcal{L}_{SFT}(\theta) = \mathbb{E}_{q \sim P(Q),\,o \sim \pi_{\theta_\text{teacher}}^{(+)}(O|q)}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}\log\pi_{\theta}(o_{t}|q,o_{\lt t}) - \beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right]\]

<p>Here, $\pi_{\theta_\text{teacher}}^{(+)}(O|q)$ indicates that only positive samples from the teacher’s generations are selected.
This supervised learning loss guides the model to follow the distribution of only the positive samples.</p>

<p>When compared with our approximated off-policy GRPO loss equation, it can be seen that the off-policy GRPO also leverages negative samples.
In Off-policy GRPO, the model learns to move toward positive samples and away from negative ones.
This property strengthens the model’s reasoning ability by steering it away from logically incorrect rollout trajectories. 
We support this claim with the following experiment.</p>

<h3 id="lets-try-out-our-experiment">Let’s try out our experiment</h3>

<p>We conducted an experiment to verify whether the off-policy GRPO method is more effective in enhancing reasoning ability than supervised fine-tuning (SFT), which only learns from positive samples. 
We used OpenThinker3-7B as our base model, and sampled approximately 2,000 Math problems from OpenThoughts3 as training data. 
The implementation was built on top of the VERL framework. 
For the training, we used a learning rate of 1e-7, a batch size of 32, 3 epochs, and a KL coefficient of 0.1. 8 samples were used per problem during training.</p>

<p>Using AIME24 as the validation set to measure performance at each epoch, the best-performing checkpoint across the three epochs was selected, and final performance was evaluated on AIME25-1 and AIME25-2. The experimental results are as follows:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>SFT</td>
      <td><strong>55.83</strong></td>
      <td>57.4</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td><strong>63.02</strong></td>
    </tr>
  </tbody>
</table>

<p>We show that Off-policy GRPO outperforms SFT by allowing the model to learn from negative samples as well.
While all training data are treated as positive in SFT, we improve the supervised learning setup by incorporating more informative reward signals through majority voting and off-policy GRPO.</p>

<p>In this experiment, we have demonstrated the effectiveness of Off-policy GRPO in further enhancing the reasoning ability of existing SOTA LLMs. We believe such method has potential to be applied to various other domains that require reasoning in the future.</p>

<h2 id="proposed-loss-for-rlvr-challenges--solutions">Proposed Loss for RLVR: Challenges &amp; Solutions</h2>
<h3 id="challenges-considering-all-positive-reasoning-trace">Challenges: Considering all positive reasoning trace</h3>
<p>A key issue arises when naively applying GRPO’s advantage calculation in the off-policy setting.
When all rollout trajectories are positive and the mean reward is 1, the resulting advantage becomes 0. 
This means that no updates are made for those samples.</p>

<p>In the case of on-policy GRPO, these all-positive samples are not problematic because the model is already performing well, making further learning from such easy problems unnecessary.</p>

<p>However, in the off-policy setting, ignoring all-positive samples can be detrimental.
In our setup, the off-policy samples are sourced from a teacher model.
Even if all teacher samples are positive, this does not necessarily mean the target model can solve the same problems perfectly.
The current objective fails to capture this discrepancy, missing valuable learning opportunities from high-quality rollout trajectories.</p>

<p>The figure below shows this discrepancy in our dataset.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/plot_ot3_non_all_positive_pie.png" style="display: inline-block; width: 40%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 2. Statistics that although teacher can generate all positive answers, but base model cannot perfectly answer. </figcaption>
</figure>

<h3 id="proposed-method">Proposed method</h3>
<p>To address the issue described above, we propose a simple modification to the loss term by adding a bias term $b$ to the advantage when all rollout trajectories are positive. 
This ensures that updates are made even when all samples are positive, thus enabling learning from fully positive reasoning traces. 
Specifically, the modified loss equation is as follows:</p>

\[\mathcal{L}_{ours}(\theta) =
\begin{cases}
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t}(\hat{A}_{i,t} + b), \max(\pi_{\theta}^{i,t}, 1-\epsilon)(\hat{A}_{i,t} + b)\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{if all } r_{i} &gt; 0 \\
\\
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t} \hat{A}_{i,t}, \max(\pi_{\theta}^{i,t}, 1-\epsilon)\hat{A}_{i,t}\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{otherwise}
\end{cases}\]

<p>For our experiments, we set $b=0.5$.</p>

<h3 id="lets-try-out-our-experiment-1">Let’s try out our experiment</h3>

<p>In this experiment, we demonstrate that the proposed method improves performance over the existing off-policy GRPO. 
Furthermore, we also conducted additional experiments comparing performance with on-policy GRPO. For on-policy GRPO, we generated 8 rollout samples per problem during training.</p>

<p>We used the exact same experimental setup as in the first experiment, changing only the training objective.
The same model (OpenThinker3-7B), the same data (2,000 math problems from OpenThoughts3), and the same hyperparameters (learning rate=1e-7, batch size=32, epochs=3, KL coefficient=0.1, 8 samples per problem) were used. 
The bias term was set to 0.5.</p>

<p>Performance was evaluated in the same manner—selecting the best checkpoint using AIME24 as the validation set and then evaluating final performance on AIME25-1 and AIME25-2. The experimental results are as follows:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>On-policy GRPO</td>
      <td>55.42</td>
      <td>59.58</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td>63.02</td>
    </tr>
    <tr>
      <td>Off-policy GRPO (with all positive group bias)</td>
      <td><strong>56.35</strong></td>
      <td><strong>63.85</strong></td>
    </tr>
  </tbody>
</table>

<p>The experimental results show that the proposed modified off-policy GRPO outperforms not only the existing off-policy GRPO but also on-policy GRPO. 
We believe these results imply the intrinsic limitation of the on-policy method, its performance being limited by the base model, as mentioned earlier.</p>

<p>In conclusion, we show that the proposed modified loss successfully addresses the limitation of existing off-policy GRPO in learning from all-positive samples, thereby further improving the model’s reasoning performance.
We believe this modification can also be effective in domains beyond mathematics.</p>

<h2 id="generalization-across-multiple-models-and-benchmarks">Generalization Across Multiple Models and Benchmarks</h2>

<p>We further investigated whether our proposed continual post-training methodology consistently improves performance across a broader range of models and various benchmarks.
To this end, we maintained the previous experimental setup and additionally evaluated two additional models: OpenThinker2-7B and AceReason-Nemetron-1.1-7B.
We also extended our evaluation to a broader range of benchmarks: AIME25 and AMC23 for mathematical problem-solving skills, LiveCodeBench for code generation ability, GPQA-Diamond for general reasoning ability, and IFEval for instruction-following.
We used the <a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> framework for AIME25, AMC23, LiveCodeBench, and GPQA-Diamond, and the <a href="https://github.com/EleutherAI/lm-evaluation-harness">llm-evaluation-harness</a> for IFEval.</p>

<p>The experimental results are presented in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>AIME25</th>
      <th>AMC23</th>
      <th>LiveCodeBench</th>
      <th>GPQA-Diamond</th>
      <th>IFEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Openthinker3-7B</td>
      <td>Base</td>
      <td>57.2915</td>
      <td>92.617</td>
      <td>63.968</td>
      <td>50.947</td>
      <td>50.09</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>59.5315</td>
      <td>93.516</td>
      <td>64.995</td>
      <td>49.684</td>
      <td>51.66</td>
    </tr>
    <tr>
      <td>Openthinker2-7B</td>
      <td>Base</td>
      <td>39.792</td>
      <td>88.633</td>
      <td>56.115</td>
      <td>45.833</td>
      <td>53.3</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>40.3645</td>
      <td>87.656</td>
      <td>55.944</td>
      <td>46.843</td>
      <td>52.20</td>
    </tr>
    <tr>
      <td>AceReason-Nemetron-1.1-7B</td>
      <td>Base</td>
      <td>64.635</td>
      <td>92.93</td>
      <td>72.383</td>
      <td>52.462</td>
      <td>36.02</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>65.521</td>
      <td>93.164</td>
      <td>72.603</td>
      <td>54.356</td>
      <td>38.23</td>
    </tr>
  </tbody>
</table>

<p>As the models were post-trained on math data, all models showed consistent performance improvements on the math-related benchmarks (AIME25, AMC23).
Interestingly, performance in other domains was maintained without degradation compared to the original models, and even showed a slight improvement on the code generation benchmark, LiveCodeBench.
These results indicate an absence of any catastrophic forgetting during the continual post-training process.</p>

<p>Additionally, off-policy GRPO is highly efficient as it recycles the teacher model’s trajectories without direct rollout during the training process. 
We therefore achieved performance comparable to or better than existing reinforcement learning methods while significantly reducing computational cost.
Being compute-efficient yet strong in performance, we believe that our proposed continual post-training methodology is attractive to many practitioners, and expect it to be useful across various domains and applications.
Finally, we have publicly released all the evaluated models on <a href="https://huggingface.co/KRAFTON">huggingface</a> to contribute to the open-source community and to make our results reproducible.</p>]]></content><author><name>KRAFTON</name></author><summary type="html"><![CDATA[In this post, we explore a new approach to enhancing the reasoning capabilities of LLMs through continual post-training. While pre-training equips LLMs with broad linguistic knowledge, it often falls short in complex reasoning tasks like math or code. Recent models have shown that Reinforcement Learning with Verifiable Rewards (RLVR) can help bridge this gap, but existing methods rely on slow and limited on-policy training. We propose an off-policy alternative using teacher-generated trajectories and introduce a novel variant of Group Relative Policy Optimization (GRPO) that better captures high-quality reasoning traces—even when all outputs are positive. Our experiments on mathematical reasoning show that this method leads to consistent improvements.]]></summary></entry><entry><title type="html">[KR] Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning</title><link href="https://krafton-ai.github.io/blog/llm_post_training_kr/" rel="alternate" type="text/html" title="[KR] Continual Post-Training of LLMs via Off-Policy GRPO for Mathematical Reasoning" /><published>2025-07-28T00:00:00+08:00</published><updated>2025-07-28T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/llm_post_training_kr</id><content type="html" xml:base="https://krafton-ai.github.io/blog/llm_post_training_kr/"><![CDATA[<h1 id="boosting-math-reasoning-in-open-source-llms-via-continual-post-training">Boosting Math Reasoning in Open-Source LLMs via Continual Post-Training</h1>
<!-- Affliation: KRAFTON & SKT -->
<p>우리는 여러 추론형 Large Language Models (LLMs) 들의 수학 추론 성능을 더 끌어올리 위해 활용할 수 있는 continual post-training 방법을 제안했습니다.
기존에 많이 활용되는 on-policy GRPO는 매 step마다 model에서 rollout을 통해 sample을 뽑아야 하기 때문에 rollout 과정에서 많은 시간이 소요될 뿐 아니라, 학습 초기의 base LLM 성능이 좋지 않을 경우 높은 reward를 갖는 좋은 sample 자체를 거의 얻을 수 없다는 문제가 있습니다.
이러한 단점을 해결하기 위해, 미리 준비해둔 teacher의 reasoning trajectory만을 활용하여 off-policy GRPO를 진행합니다. 
추가적으로, 기존 GRPO는 group 내 모든 sample이 positive일 경우 advantage가 0이 되어 학습이 이루어지지 않는데, 여기선 teacher가 생성한 trajectory만을 다룸으로 모두 positive이더라도 supervision을 받을 수 있도록 GRPO의 advantage에 bias term을 추가하는 방식을 제안합니다.
최종적으로, Figure 1 과 같이 수학에 더 특화된 성능을 얻은 LLM들을 얻을 수 있었습니다.
또한, 이 학습 방법으로 수학이 아닌 다른 task에서는 기존 모델과 유사한 성능을 유지할 수 있음을 확인했습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/radar_charts.png" style="display: inline-block; width: 100%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 1: Performance Comparision between base LLMs and base LLMs with our method.</figcaption>
</figure>

<h2 id="why-do-we-focus-on-post-training-for-reasoning">Why do we focus on post-training for reasoning?</h2>
<p>Large text corpus로 학습하는 LLM의 사전학습(pre‑training)은 언어의 통계적 패턴과 구조를 이해하는 데 큰 역할을 합니다.
하지만 단계를 거쳐야 하는 복잡한 논리 추론, 수학적 문제 해결, 또는 코드 작성과 같은 작업에서는 충분하지 않습니다.
예컨대 Chain-of-Thought (CoT)를 쓰면 어느 정도 추론 과정을 모방하게 할 수 있지만, 이는 모델이 일관된 사고 흐름을 자기 것으로 내재화한 것이 아니라 성능의 한계가 존재합니다.</p>

<p>이런 한계를 극복하기 위한 post-training 방법 중 하나로 Reinforcement Learning with Verifiable Rewards (RLVR) 이 있습니다.
이 방식의 post-training이 LLM의 추론 강화에 효과적이라는 사실은 DeepSeek-R1 이나 OpenAI의 o1/o3 모델과 같은 모델들이 RLVR 방식을 통해 실질적인 추론 성능향상을 보이면서 더 각광을 받게 되었습니다.
RLVR은 LLM이 생성한 전체 추론 과정인 chain-of-thought(CoT) 형태의 rollout trajectory가 검증 가능해야 합니다.
예를 들어, 수학 문제의 정답이 있어서 최종 정답이 비교가 되거나, 각 단계에 대한 평가가 가능해, 생성된 rollout trajectory 전체에 대해 보상(reward)을 줄 수 있는 구조입니다.
이런 보상을 바탕으로, RL 알고리즘(e.g., PPO, GRPO)을 통해, LLM은 검증된 추론 과정을 더 많이 생성하도록 학습되게 됩니다.</p>

<p>대표적인 RLVR 방식의 학습 알고리즘으로는 Group Relative Policy Optimization (GRPO) 방식 <d-cite key="guo2025deepseekR1"></d-cite>이 있습니다.
이 방식은 동일 질문에 대해 샘플링한 여러 rollout trajectory들의 그룹의 평균 보상을 기준으로 삼아 각 응답의 상대적 advantage를 계산합니다.
이의 objective $\mathcal{L}$을 수식으로 표현하면 아래와 같습니다:</p>

\[\mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{old}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>여기서, $\pi^{i,t} = \pi(o_{i,t}\mid q, o_{i,&lt;t})$이며, $G$는 한 그룹 내 응답 개수를 의미합니다. 또한, $\pi_\text{ref}$, $\pi_{\theta}$, $\pi_\text{old}$는 각각 기준(reference) LLM, 학습 중인 LLM, 샘플링에 사용되는 LLM을 나타냅니다. $q$와 ${o_i}_{i=1}^{G}$는 학습에 사용되는 질문과 생성된 rollout trajectory 세트를 나타냅니다.
각 응답의 상대적 advantage인 $\hat{A}_i$는 다음과 같이 계산됩니다.</p>

\[\hat{A}_i = \frac{r_i - \text{mean}(r_i)}{\text{std}(r_i)}\]

<p>이렇게 계산된 trajectory level 의 advantage는 응답의 각 토큰 수준에 적용되어 최종적으로 $\hat{A}_{i,t}$로 사용됩니다.</p>

<h2 id="off-policy-reinforcement-learning-with-verifiable-reward-off-policy-rlvr">Off-policy Reinforcement Learning with Verifiable Reward (Off-policy RLVR)</h2>
<h3 id="why-do-we-focus-on-off-policy-rlvr-eg-grpo">Why do we focus on Off-policy RLVR (e.g., GRPO)?</h3>
<p>앞서 설명한 RLVR 의 경우는 on-policy 로 advantage를 업데이트 하는 방식입니다.
On-policy 의 경우, 학습하는 모델이 뽑아낸 roll-out trajectory 에 대해 reward 를 계산해 이를 통해 LLM을 업데이트 한다는 것을 의미합니다.</p>

<p>하지만, on-policy로 학습하는 것에는 두가지 단점이 존재합니다.
첫번째 단점은 학습 속도가 매우 느리다는 것입니다. <d-cite key="liang2025squeeze"></d-cite>
이 이유는 매 스텝 loss 를 계산할 때 roll-out 을 각 문제마다 $G$ 개수만큼 해야하기 때문입니다.
또한, on-policy 는 기존 base LLM 성능에 한계가 제한됩니다. <d-cite key="luffy"></d-cite>
기존 base LLM 의 성능이 낮으면 정답 trajectory 를 뽑을 확률이 낮아져 성능 향상의 bottleneck 이 생길 수 있기 때문입니다.</p>

<p>On-policy 의 문제들을 해결하기 위해, 저희는 정답 label 을 뽑기 위해 쓴 teacher LLM의 roll-out trajectory 를 활용하는 off-policy RLVR 방식을 도입했습니다.
저희가 이번 프로젝트에 활용한 데이터는 OpenThought3 <d-cite key="openthoughts"></d-cite> 데이터셋입니다.
OpenThought3는 Math, Code, Science 등에 관련된 문제들에 대해 teacher model 인 QwQ-32B 모델을 이용해 sampling 을 16번씩 진행해 만든 데이터입니다.
이 중, 저희는 정답이 확실한 Math 에 대해서만 먼저 filtering 하여 준비했습니다.
그 뒤, 각 문제마다 Majority Voting <d-cite key="wangself"></d-cite> 을 이용해 정답으로 간주될 수 있는 답들을 찾고, 이를 바탕으로 각 trajectory 의 reward 를 계산하였습니다.</p>

<p>이 때, Teacher model 을 $\theta_\text{teacher}$ 라고 하고, teacher가 정답 label을 뽑기 위해 roll-out 한 문제들의 random variable을 $Q$ 라고 할 때, 우리는 off-policy GRPO를 아래와 같이 수식을 쓸 수 있습니다:</p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>이 때, practical 하게 teacher 의 probability 를 모르기 때문에, $\pi_{\theta_\text{teacher}}^{i,t}=1$ 로 가정하였습니다.
그러면 항상 $\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}} \le 1$ 이기 때문에, $\text{clip}$ 은 $\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)$ 로 표현될 수 있습니다.
최종적으로, practical하게 off-policy 에 해당하는 GRPO 수식은 아래와 같이 근사될 수 있습니다. <d-cite key="luffy"></d-cite></p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\pi_{\theta}^{i,t}\hat{A}_{i,t},\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<h3 id="off-policy-grpo-vs-supervised-fine-tuning-sft">Off-policy GRPO vs. Supervised Fine-tuning (SFT)</h3>

<p>제일 간단하게 off-policy 로 준비된 teacher sample 로 학습한다는 것은 SFT를 하는 것으로 생각할 수 있습니다.
그러면 SFT 와 Off-policy GRPO 는 무슨 차이가 있을까요?
아래의 표현된 SFT 수식과 off-policy GRPO 의 수식을 비교하면 그 차이를 알 수 있습니다.</p>

\[\mathcal{L}_{SFT}(\theta) = \mathbb{E}_{q \sim P(Q),\,o \sim \pi_{\theta_\text{teacher}}^{(+)}(O|q)}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}\log\pi_{\theta}(o_{t}|q,o_{\lt t}) - \beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right]\]

<p>여기서, $\pi_{\theta_\text{teacher}}^{(+)}(O|q)$는 teacher가 생성한 샘플 중에서 positive reward를 갖는 샘플만 선택했음을 나타냅니다.
이를 통해 모델은 오로지 positive 샘플의 분포를 따라가도록 지도학습을 수행하게 됩니다.</p>

<p>Practical하게 근사한 off-policy GRPO 수식과 비교했을 때, 우리의 Off-policy GRPO 수식은 negative 에 해당하는 sample 또한 반영된다는 점입니다.
Off-policy GRPO 에서는 positive 와는 계속 가까워지게 학습을 하며, 반대로 negative 와는 멀어지게 학습을 하는 구조가 됩니다.
이로 인해 논리적으로 틀려 가지 않아야 할 rollout trajectory 에 멀어지게 되어, 더 추론 능력이 강화될 수 있습니다.
이 사실을 아래 실험을 통해 확인해 보았습니다.</p>

<h3 id="lets-try-out-our-experiment">Let’s try out our experiment</h3>

<p>우리는 off-policy GRPO 방식이 단순히 positive 샘플만을 학습하는 supervised fine-tuning (SFT) 방식보다 추론 능력 강화에 더 효과적인지 확인하기 위해 실험을 수행했습니다. 
실험에 사용된 모델은 OpenThinker3-7B이며, 데이터는 OpenThought3의 Math 문제 중 약 2,000개를 sampling하여 구성했습니다. 
실험 환경은 VERL 프레임워크를 기반으로 구축되었고, 학습 과정에서 learning rate는 1e-7, batch size는 32, epoch는 3, KL coefficient는 0.1로 설정했습니다. 이때 각 문제당 8개의 샘플을 사용하여 학습을 진행했습니다.</p>

<p>성능 평가는 AIME24를 validation set으로 사용하여 각 epoch마다 성능을 측정했고, 3 epoch 중 가장 우수한 성능을 보인 checkpoint를 선정하여 최종적으로 AIME25-1과 AIME25-2에서 성능을 비교했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>SFT</td>
      <td><strong>55.83</strong></td>
      <td>57.4</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td><strong>63.02</strong></td>
    </tr>
  </tbody>
</table>

<p>Off-policy GRPO 방식이 SFT보다 종합적으로 더 뛰어난 성능을 보인 이유는 negative sample들로부터 모델이 적극적으로 멀어지도록 학습을 유도했기 때문입니다. 
기존의 데이터는 모두 positive sample로 고려되고 있지만, 본 실험에서는 majority voting과 Off-policy GRPO를 통해 보다 정확한 정보를 확보하고 이를 바탕으로 negative sample로부터 벗어나도록 유도하여, 더욱 세밀한 지도학습이 가능했습니다.</p>

<p>본 실험을 통해 Off-policy GRPO 방식이 기존 SOTA LLM의 논리적 추론 능력을 한 단계 더 향상시키는 데 매우 효과적임을 입증했습니다. 이 방식은 향후 다양한 추론 도메인에서도 충분히 적용 가능할 것으로 기대됩니다.</p>

<h2 id="proposed-loss-for-rlvr-challenges--solutions">Proposed Loss for RLVR: Challenges &amp; Solutions</h2>
<h3 id="challenges-considering-all-positive-reasoning-trace">Challenges: Considering all positive reasoning trace</h3>
<p>Off-policy GRPO 는 그럼 문제가 없을까요?
아닙니다. 이유는 roll-out trajectory 가 all positive인 sample의 Advantage 값에 있습니다.
실제로 어떤 quesiton에 대해서는 전부 다 positive 인 sample 이 존재 할 수 있습니다.
이런 데이터의 경우 advantage 값을 계산하게 되면 mean reward 가 1 이 되어 0이 되고, 그러면 그 sample 들에 대해서는 update 가 되지 않습니다.</p>

<p>On-policy 의 경우에서는 이런 all positive sample 들이 부분이 문제가 안됩니다.
왜냐하면 이미 모델이 잘하는 경우라 update 를 하지 않아도 되기 때문입니다.</p>

<p>하지만, 반대로 Off-policy 의 경우에는 all positive sample 들을 학습하지 않는 것이 도움이 되지 않는 경우가 있습니다.
Off-policy 로 준비한 sample 이 teacher sample 인 저희의 case 의 경우를 생각해 보겠습니다.
이 경우, teacher sample 은 전부 다 positive 여서 reward 가 1이지만, 실제 그 문제에 대해 저희가 학습할 target 모델은 틀린 sample 을 생성할 가능성이 높을 수 있습니다.
하지만 현재 Objective 로는 이 부분을 고려하지 못하고, 결국 배워야 할 rollout trajectory 에 대해 학습하지 못함을 의미합니다.</p>

<p>저희가 준비한 데이터에서 이 문제가 있음을 아래 그림을 통해 확인이 되었습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/plot_ot3_non_all_positive_pie.png" style="display: inline-block; width: 40%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 2. Statistics that although teacher can generate all positive answers, but base model cannot perfectly answer. </figcaption>
</figure>

<h3 id="proposed-method">Proposed method</h3>
<p>위 문제를 해결하기 위해, 저희는 all positive roll-out trajectory 인 경우에는 advantage 에 bias term 인 $b$를 추가하는 simple 한 변형 loss term 을 제안하였습니다.
제안 방식을 통해 all positive 인 sample 이 나올 때도 그 sample 들에 대해 update 가 되도록 해주어서, all positive reasoning trace를 반영하지 못하는 문제를 해결합니다.
자세한 수식은 아래와 같습니다.</p>

\[\mathcal{L}_{ours}(\theta) =
\begin{cases}
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t}(\hat{A}_{i,t} + b), \max(\pi_{\theta}^{i,t}, 1-\epsilon)(\hat{A}_{i,t} + b)\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{if all } r_{i} &gt; 0 \\
\\
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t} \hat{A}_{i,t}, \max(\pi_{\theta}^{i,t}, 1-\epsilon)\hat{A}_{i,t}\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{otherwise}
\end{cases}\]

<p>저희는 $b=0.5$ 로 setting 을 하고 실험을 진행했습니다.</p>

<h3 id="lets-try-out-our-experiment-1">Let’s try out our experiment</h3>

<p>본 실험에서는 제안한 방식이 기존 off-policy GRPO보다 성능 향상에 도움이 되는지 추가로 검증하고자 하였습니다. 
특히, On-policy GRPO와의 성능 차이도 함께 비교하기 위해 추가적으로 실험을 진행하였습니다. On-policy GRPO 방식의 경우 각 문제마다 동일하게 8개의 샘플을 직접 roll-out하여 학습을 진행했습니다.</p>

<p>실험 환경은 첫 번째 실험과 완벽히 동일하며, 오직 학습 objective만 달라졌습니다. 
동일한 모델(OpenThinker3-7B), 동일한 데이터(OpenThought3 Math 문제 2,000개), 동일한 hyperparameter(learning rate=1e-7, batch size=32, epoch=3, KL coefficient=0.1, 각 문제당 8개의 샘플)를 사용했습니다. 
또한 bias term은 0.5로 설정하여 진행했습니다.</p>

<p>성능 평가는 동일한 방식으로 AIME24를 validation set으로 사용하여 best checkpoint를 선정한 후, 최종적으로 AIME25-1과 AIME25-2를 통해 성능을 평가했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>On-policy GRPO</td>
      <td>55.42</td>
      <td>59.58</td>
    </tr>
    <tr>
      <td>Off-policy GRPO</td>
      <td>55.21</td>
      <td>63.02</td>
    </tr>
    <tr>
      <td>Off-policy GRPO (with all positive group bias)</td>
      <td><strong>56.35</strong></td>
      <td><strong>63.85</strong></td>
    </tr>
  </tbody>
</table>

<p>실험 결과, 제안한 modified off-policy GRPO 방식이 기존 off-policy GRPO 방식뿐만 아니라 On-policy GRPO 방식보다도 더 높은 성능을 기록했습니다. 
이는 앞서 언급된 바와 같이, On-policy 방식이 가진 base model 성능에 의해 제한된다는 intrinsic한 단점 때문으로 추측됩니다.</p>

<p>종합적으로, 제안한 modified loss 방식이 기존 off-policy GRPO의 한계였던 all-positive 샘플의 학습 문제를 성공적으로 극복했으며, 이를 통해 추론 성능을 더욱 강화할 수 있음을 확인했습니다. 
향후 다른 다양한 추론 도메인에서도 이 방법론이 효과적으로 활용될 가능성이 클 것으로 기대됩니다.</p>

<h2 id="generalization-across-multiple-models-and-benchmarks">Generalization Across Multiple Models and Benchmarks</h2>

<p>우리는 제안한 continual post-training 방법론이 더 광범위한 모델과 다양한 벤치마크에서도 일관된 성능 개선을 보이는지 추가적으로 확인해 보았습니다. 
이를 위해, 앞선 실험의 세팅을 유지한 상태에서 추가적으로 두 가지 모델(Openthinker2-7B, AceReason-Nemetron-1.1-7B)을 사용해 평가했습니다. 
또한 평가를 위해 선택한 벤치마크는 수학 문제 해결 능력을 평가하는 AIME25와 AMC23, 코드 생성 능력을 평가하는 LiveCodeBench, 일반적 추론 능력을 평가하는 GPQA-Diamond, 그리고 사실 검증 성능을 평가하는 IFEval입니다.
여기서 AIME25와 AMC23, LiveCodeBench, GPQA-Diamond 평가에는 <a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> framework, IFEval 평가엔 <a href="https://github.com/EleutherAI/lm-evaluation-harness">llm-evaluation-harness</a> framework을 사용하였습니다.</p>

<p>실험 결과는 아래 표와 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>AIME25</th>
      <th>AMC23</th>
      <th>LiveCodeBench</th>
      <th>GPQA-Diamond</th>
      <th>IFEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Openthinker3-7B</td>
      <td>Base</td>
      <td>57.2915</td>
      <td>92.617</td>
      <td>63.968</td>
      <td>50.947</td>
      <td>50.09</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>59.5315</td>
      <td>93.516</td>
      <td>64.995</td>
      <td>49.684</td>
      <td>51.66</td>
    </tr>
    <tr>
      <td>Openthinker2-7B</td>
      <td>Base</td>
      <td>39.792</td>
      <td>88.633</td>
      <td>56.115</td>
      <td>45.833</td>
      <td>53.3</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>40.3645</td>
      <td>87.656</td>
      <td>55.944</td>
      <td>46.843</td>
      <td>52.20</td>
    </tr>
    <tr>
      <td>AceReason-Nemetron-1.1-7B</td>
      <td>Base</td>
      <td>64.635</td>
      <td>92.93</td>
      <td>72.383</td>
      <td>52.462</td>
      <td>36.02</td>
    </tr>
    <tr>
      <td> </td>
      <td>Off-policy GRPO (+bias)</td>
      <td>65.521</td>
      <td>93.164</td>
      <td>72.603</td>
      <td>54.356</td>
      <td>38.23</td>
    </tr>
  </tbody>
</table>

<p>실험 결과를 통해 Math 데이터로 post-training한 만큼 수학 관련 벤치마크(AIME25, AMC23)에서는 모든 모델에서 일관된 성능 향상이 관찰되었습니다. 
흥미롭게도, 다른 도메인에서의 성능 또한 기존 대비 저하되지 않고 유지되었으며, 특히 코드 생성 벤치마크인 LiveCodeBench에서는 오히려 소폭의 성능 향상이 관찰되었습니다. 
이는 continual post-training 과정에서도 catastrophic forgetting이 거의 발생하지 않았다는 것을 의미합니다.</p>

<p>추가적으로, off-policy GRPO 방식은 실제 학습 과정에서 직접적인 rollout 없이 teacher model의 trajectory를 재활용하기 때문에 매우 효율적입니다. 
이러한 효율성 덕분에 기존의 reinforcement learning 방식 대비 계산 자원을 현저히 절약하면서도, 성능은 동등하거나 더 뛰어난 결과를 달성했습니다. 
따라서 저희가 제안한 continual post-training 방법론은 성능과 효율성을 모두 갖춘 실용적인 접근법으로, 향후 다양한 도메인과 응용 분야에서도 매우 유용하게 활용될 수 있을 것으로 기대합니다.
마지막으로, 오픈 소스 커뮤니티에 기여하고 연구의 재현성과 확장 가능성을 높이기 위해 <a href="https://huggingface.co/KRAFTON">huggingface</a>에 평가한 모델들을 모두 공개하였습니다.</p>]]></content><author><name>KRAFTON</name></author><summary type="html"><![CDATA[이 포스팅에서는 거대 언어 모델(LLM)의 추론 능력을 지속적인 추가 학습(Continual Post-training)을 통해 향상시키는 새로운 접근법을 소개합니다. 기존의 사전 학습(Pre-training)은 광범위한 언어적 지식을 제공하지만, 수학이나 코드 생성과 같은 복잡한 추론 작업에서는 여전히 한계를 드러냅니다. 최근 연구에서는 강화학습 기반의 검증 가능한 보상 기법(RLVR)이 이러한 한계를 어느 정도 해소할 수 있다고 밝혔으나, 현재까지 제안된 방법들은 느리고 제한적인 온-폴리시(On-policy) 학습에 의존하고 있습니다. 우리는 이 문제를 해결하기 위해 교사 모델이 생성한 우수한 추론 경로(Trajectory)만을 활용하는 오프-폴리시(Off-policy) 학습 방식을 제안하며, 특히 모든 생성된 결과가 긍정적인 경우에도 고품질의 추론 과정을 효과적으로 학습할 수 있도록 GRPO 알고리즘의 새로운 변형 방식을 도입했습니다. 수학적 추론을 대상으로 한 실험 결과, 제안한 방법이 일관된 성능 향상을 보임을 확인하였습니다.]]></summary></entry></feed>