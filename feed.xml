<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://krafton-ai.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://krafton-ai.github.io/blog/" rel="alternate" type="text/html" hreflang="en" /><updated>2026-02-20T18:38:40+08:00</updated><id>https://krafton-ai.github.io/blog/feed.xml</id><title type="html">Krafton AI</title><subtitle>Home to the Krafton AI Blog
</subtitle><entry><title type="html">[EN] How We Reached 74.8% on terminal-bench with Terminus-KIRA</title><link href="https://krafton-ai.github.io/blog/terminus_kira_en/" rel="alternate" type="text/html" title="[EN] How We Reached 74.8% on terminal-bench with Terminus-KIRA" /><published>2026-02-20T00:00:00+08:00</published><updated>2026-02-20T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/terminus_kira_en</id><content type="html" xml:base="https://krafton-ai.github.io/blog/terminus_kira_en/"><![CDATA[<p><img src="/blog/assets/img/2026-02-20-terminus_kira/terminus_kira_1.jpeg" style="width: 100%; height: auto;" /></p>

<h1 id="how-we-reached-748-on-terminal-bench-with-terminus-kira">How We Reached 74.8% on terminal-bench with Terminus-KIRA</h1>

<h2 id="1-preliminaries-what-is-terminal-bench-what-is-terminus">1. Preliminaries: What is terminal-bench? What is Terminus?</h2>

<p><strong><em>terminal-bench</em></strong> is a benchmark for evaluating how well AI agents can solve real tasks by operating in a terminal environment. Even though it says “terminal,” it includes lots of interesting math/ML problems, so it’s basically: “can AI agents do what ML/AI/SW engineers do :P?” It matters because terminal-based work is where a lot of real engineering happens. Debugging, coding, running experiments, and shipping fixes end-to-end.</p>

<p><strong><em>Terminus</em></strong> is an agent harness that lets an LLM interact with a real terminal (via tmux), observe outputs, and iterate until it decides to submit a final answer. Compared to other “in-machine” harnesses that directly interact with the virtual machine via various tools, Terminus is intentionally minimal: it’s a clean loop of “send command → read buffer → think → repeat”.</p>

<p>Most frontier AI labs equip their models with Terminus 2 and report the evaluation results on terminal-bench. Conceptually it’s very simple and elegant, and it’s tempting to believe that a good enough agent should be able to get the maximum accuracy out of it in the end.</p>

<p>But that needs some fixing…</p>

<h2 id="2-reality">2. Reality</h2>

<p>However, if you actually run various agents on terminal-bench and see how it fails, then you see a lot of interesting failure modes. We developed our in-house agent behavior analyzer, i.e., an automated analysis of what agents are good at, what agents are bad at, and analyzing the success/failure behaviors, etc.</p>

<p>The analysis results are pretty shocking.</p>

<p><strong>TL;DR:</strong> Terminus 2 being an absolutely minimal harness makes highly capable models make so many mistakes that can be avoided.</p>

<p>Here are the most important takeaway messages from our analysis of Terminus 2 + frontier models.</p>

<h3 id="2-1-models-are-optimized-to-assist-humans-not-to-totally-replace-humans">2-1. Models are optimized to “assist” humans, not to totally replace humans</h3>

<p>I think this is the most important message I want to deliver in this report. Models are mostly trained to “assist” us (us = humans) by interacting with us, not to complete the whole task on their own.</p>

<p><strong>- Models tend to submit partial work and “give it a shot”</strong></p>

<p>This is totally fine in the traditional “assistant” setup, and indeed it was a desired behavior. The models show us partial results so that we (humans) can give feedback to them early on. However, this is a bad thing for terminal-bench (or any longer-horizon task) where we expect agents to complete the whole task with perfection. When we put these models in an agent loop that only weakly steers their behaviors toward completion, they still tend to return semi-complete results, especially when the task is very difficult and requires long-horizon planning. Terminus 2 is so minimal that it fails to guide the model to complete the whole task with high probability.</p>

<p><strong>- It tends to show some visuals to humans</strong></p>

<p>This is my favorite finding. Recent LLMs all do have “eyes,” and they are post-trained to believe they have eyes. However, because they still were trained to assist us. When tasks require complex visual inspection or understanding, the models often hand off those hard visual-understanding parts to us, ugh!!! Terminus 2 doesn’t prevent this hand-off behavior.</p>

<h3 id="2-2-models-are-still-not-perfect">2-2. Models are still not perfect</h3>

<p><strong>- Bad self-evaluation (false completion)</strong></p>

<p>Models are really bad at self-evaluating their own output. When the model thinks it’s over, Terminus 2 is designed to ask back “are you sure?” And it usually just says “yup, I am sure :-)” even if their work is not completed or wrong. This causes a lot of “false completion” errors.</p>

<p><strong>- Bad adaptive replanning</strong></p>

<p>There are genuine bottlenecks in today’s frontier models. They are pretty good at planning from the beginning given all the information, but after they observe new information, the models struggle at adjusting their old plans.</p>

<h3 id="2-3-terminal-bench-and-terminus-specific-failure-modes">2-3. Terminal-bench and Terminus-specific failure modes</h3>

<p>For instance, sometimes the problem is vague in terms of how general the solution should be. Should an agent need to build a bullet-proof solution that works in an arbitrarily different environment or it is okay to check if this solution runs well in the current environment? That’s not clear in most cases. Agents tend to find a solution that answers questions in a narrower sense. This would be a question where “good” students would have raised their hand and had a chance to clarify during an exam! However, there is no such luxury for the agents. This makes agents make a random guess between the narrower/stricter problem specifications.</p>

<p>By the TerminalBench2 rule, agents don’t know how much time they are given. This makes some agents install huge libraries and tools, wasting too much time installing dependencies.</p>

<p><strong>There are also two critical specific limitations to Terminus:</strong></p>

<ul>
  <li>
    <p>Terminus uses a “push and wait” mechanism, i.e., it sends a command to underlying tmux and waits for a “guessed” runtime. This incurs non-negligible wasted time.</p>
  </li>
  <li>
    <p>Terminus uses an internal tmux screen and reads off its buffer output and passes it to an LLM. The default buffer size is too small, making agents confused when reading large files.</p>
  </li>
</ul>

<h2 id="3-introducing-terminus-kira">3. Introducing Terminus-KIRA</h2>

<p>We propose a few simple fixes to the challenges mentioned above.</p>

<p>To address the first challenge, we make a few changes to the agent prompt &amp; add one more tool.</p>

<ul>
  <li>
    <p>The agent prompt now clearly specifies that there is only one submission without any human interaction, and the submission is FINAL, etc. This way, we prevent agents from submitting premature answers.</p>
  </li>
  <li>
    <p>To avoid agents expecting humans to help with visual inspection, we literally add this prompt: “You must complete the entire task without any human intervention.”</p>
  </li>
  <li>
    <p>Then, since the tmux buffer cannot forward multimedia files, we add a tool that’s specifically designated for multimedia file reading/comprehension. Thus, LLMs are given two tools: (1) issuing a command, or (2) directly reading/comprehending multimedia files using the backend LLM. We also added the following prompt: “You do NOT have eyes or ears, so you MUST resort to various programmatic/AI tools to understand multimedia files.”
<img src="/blog/assets/img/2026-02-20-terminus_kira/terminus_kira_2.jpeg" style="width: 100%; height: auto;" /></p>
  </li>
</ul>

<p>We also attempt to address the second challenge (bad self-evaluation and bad replanning).</p>

<ul>
  <li>The self-completion-check part now requires a very thorough step-by-step objective self-evaluation of the progress &amp; results. We saw a significant reduction in false completion rate.
<img src="/blog/assets/img/2026-02-20-terminus_kira/terminus_kira_3.jpeg" style="width: 100%; height: auto;" /></li>
  <li>For replanning, we used a very simple prompting technique to help the model adaptively replan better.</li>
</ul>

<p>Lastly, we also added a few generic tips to the prompt, for instance:
<img src="/blog/assets/img/2026-02-20-terminus_kira/terminus_kira_4.jpeg" style="width: 100%; height: auto;" />
The first tip helps the agent come up with a broader/generic solution. The second tip helps the agent avoid installing heavy dependencies, causing timeout errors.</p>

<p>Lastly, we also modified the tmux interface. The tmux interface is updated with a “pull” mechanism. Now the agent doesn’t have to wait excess time if the predicted runtime was larger than the actual runtime. We also increased the tmux buffer size.</p>

<h2 id="4-results">4. Results?</h2>

<p>It works :)</p>

<p><img src="/blog/assets/img/2026-02-20-terminus_kira/terminus_kira_5.jpeg" style="width: 100%; height: auto;" /></p>

<p>We decided to open-source Terminus-KIRA because why not:<br />
https://github.com/krafton-ai/kira</p>

<p>Enjoy :)</p>

<h2 id="5-takeaway--prediction">5. Takeaway &amp; prediction</h2>

<p><strong>TL;DR:</strong></p>

<ul>
  <li>Terminus-KIRA boosts frontier models by 10 percentage points</li>
  <li>Our fixes turn out to be very simple yet very effective.</li>
  <li>You should use Terminus-KIRA.</li>
</ul>

<p>And I am sure we will see &gt; 80% on TerminalBench2 sooner or later just with a better harness with the current models. Just my 100 won :-)</p>

<hr />

<h2 id="acknowledgement">Acknowledgement</h2>

<p><em>All authors are listed alphabetically by last name.</em></p>

<p><strong>Agent Design &amp; Experiments</strong><br />
Minseok Choi<br />
Wooseong Chung<br />
Yun Jegal<br />
Jiho Jeon<br />
Giyoung Jung<br />
Seungjin Kwon<br />
Gisang Lee<br />
Hyogon Ryu</p>

<p><strong>Project Coordination</strong><br />
Myungseok Oh</p>

<p><strong>Infrastructure</strong><br />
Hara Kang</p>

<p><strong>Advising &amp; Writing</strong><br />
Kangwook Lee</p>

<hr />
<p><strong>KRAFTON AI &amp; Ludo Robotics</strong></p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">[EN] Continual Post-Training of LLMs via Offline GRPO for Mathematical Reasoning</title><link href="https://krafton-ai.github.io/blog/llm_post_training_en/" rel="alternate" type="text/html" title="[EN] Continual Post-Training of LLMs via Offline GRPO for Mathematical Reasoning" /><published>2025-07-28T00:00:00+08:00</published><updated>2025-07-28T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/llm_post_training_en</id><content type="html" xml:base="https://krafton-ai.github.io/blog/llm_post_training_en/"><![CDATA[<h1 id="boosting-math-reasoning-in-open-source-llms"><strong>Boosting Math Reasoning in Open-Source LLMs</strong></h1>
<!-- Affliation: KRAFTON & SKT -->
<p>We propose a continual post-training method that further enhances the mathematical reasoning performance of various Large Language Models (LLMs). 
Although online GRPO is a commonly used for post-training, it suffers from long training times and is constrained by the capabilities of the base LLM. 
To address these issues, we use an offline GRPO method, which learns from a pre-existing dataset of high-quality solutions generated by a more capable teacher model.
Additionally, we introduce a bias term in the advantage function to ensure that all positive trajectories are effectively learned. 
As a result, we developed LLMs with specialized performance in mathematics, as illustrated in Figure 1. 
We also confirmed that this training method maintains the performance of the original model on tasks other than mathematics.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/radar_charts.png" style="display: inline-block; width: 100%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 1: Performance comparison between base LLMs and our continual post-trained versions.</figcaption>
</figure>

<h2 id="why-do-we-focus-on-post-training-for-reasoning">Why do we focus on post-training for reasoning?</h2>
<p>The pre-training of LLMs on large text corpora plays a crucial role in capturing statistical patterns and language structures. 
However, this is often insufficient for tasks that require complex, multi-step logical reasoning, mathematical problem-solving, or code generation. 
For instance, while using Chain-of-Thought (CoT) can mimic the reasoning process to some extent, its performance is limited because the model has not fully internalized a coherent flow of reasoning.</p>

<p>Reinforcement Learning with Verifiable Rewards (RLVR) is designed to overcome these limitations. 
The effectiveness of this post-training approach in enhancing the reasoning capabilities of LLMs has gained prominence, with models like DeepSeek-R1 and OpenAI’s o1/o3 models demonstrating substantial improvements in reasoning performance via RLVR. 
RLVR requires the entire reasoning process generated by the LLM—a rollout trajectory in the form of Chain-of-Thought (CoT)—to be verifiable. 
For example, in a math problem with a known answer, the final solution can be directly compared, or each step can be evaluated, enabling reward assignment to the entire generated trajectory. 
Based on such rewards, RL algorithms (e.g., PPO, GRPO) are used to train the LLM to generate correct reasoning processes.</p>

<p>Group Relative Policy Optimization (GRPO) <d-cite key="guo2025deepseekR1"></d-cite>, one of the most commonly used algorithms in RLVR, calculates each response’s relative advantage based on the average reward of a group of sampled rollout trajectories for the same question. 
The objective function $\mathcal{L}$ can be expressed as follows:</p>

\[\mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{old}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>where $\pi^{i,t} = \pi(o_{i,t}\mid q, o_{i,&lt;t})$, and $G$ represents the number of responses in a group. Also, $\pi_\text{ref}$, $\pi_{\theta}$, and $\pi_\text{old}$ denote the reference LLM, the LLM being trained, and the LLM from the previous step used for sampling, respectively. $q$ and ${o_i}_{i=1}^{G}$ denote the set of questions and the generated rollout trajectories used for training. 
The relative advantage of each response $\hat{A}_i$ is calculated as follows:</p>

\[\hat{A}_i = \frac{r_i - \text{mean}(r_i)}{\text{std}(r_i)}\]

<p>This trajectory-level advantage is then applied to each token level of the response, ultimately being used as $\hat{A}_{i,t}$.</p>

<h2 id="offline-reinforcement-learning-with-verifiable-reward">Offline Reinforcement Learning with Verifiable Reward</h2>
<h3 id="why-do-we-focus-on-offline-rlvr-eg-grpo">Why do we focus on Offline RLVR (e.g., GRPO)?</h3>
<p>The RLVR method described earlier computes the advantage in an online manner.
This means that the LLM is updated based on the rewards of the rollout trajectories generated by itself.</p>

<p>However, online training has two main drawbacks. 
The first is its slow training speed. <d-cite key="liang2025squeeze"></d-cite> 
This is due to the need for the generation of $G$ rollout trajectories per question at each step of loss calculation.
Second, the performance of online methods is limited by the capabilities of the base LLM. <d-cite key="luffy"></d-cite> 
With a less-capable base LLM, we get a relatively small probability of generating a correct trajectory, which can lead to a bottleneck for performance improvement.</p>

<p>To overcome these drawbacks of online methods, we turn our attention to offline RLVR methods that utilize pre-existing rollout trajectories of teacher LLMs.
In this project, we use the OpenThoughts3 <d-cite key="openthoughts"></d-cite> dataset.
The dataset consists of 16 roll-out trajectories of the teacher model, QwQ-32B, on problems in math, code, and science.
From the dataset, we only utilize math problems.
We then apply Majority Voting <d-cite key="wangself"></d-cite> to identify potentially correct answers and assign rewards to each trajectory accordingly.</p>

<p>Denoting the teacher model $\theta_\text{teacher}$ and the random variable for the problems that the teacher has seen as $Q$, we can write the offline GRPO equation as follows:</p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>Since we do not know the likelihood of the trajectory under the teacher model in practice, we assume $\pi_{\theta_\text{teacher}}^{i,t} = 1$.
This means that $\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}} \le 1$ is always true, $\text{clip}$ can be expressed as $\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)$.
Therefore, the offline GRPO objective can be approximated as follows: <d-cite key="luffy"></d-cite></p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\pi_{\theta}^{i,t}\hat{A}_{i,t},\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<h3 id="offline-grpo-vs-supervised-fine-tuning-sft">Offline GRPO vs. Supervised Fine-tuning (SFT)</h3>

<p>The simplest way to train a model with offline data is SFT.
So, what distinguishes SFT from offline GRPO? 
The difference becomes clear by comparing the SFT loss equation below with that of offline GRPO.</p>

\[\mathcal{L}_{SFT}(\theta) = \mathbb{E}_{q \sim P(Q),\,o \sim \pi_{\theta_\text{teacher}}^{(+)}(O|q)}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}\log\pi_{\theta}(o_{t}|q,o_{\lt t}) - \beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right]\]

<p>Here, $\pi_{\theta_\text{teacher}}^{(+)}(O|q)$ indicates that only positive samples from the teacher’s generations are selected.
This supervised learning loss guides the model to follow the distribution of only the positive samples.</p>

<p>When compared with our approximated offline GRPO loss equation, it can be seen that the offline GRPO also leverages negative samples.
In Offline GRPO, the model learns to move toward positive samples and away from negative ones.
This property strengthens the model’s reasoning ability by steering it away from logically incorrect rollout trajectories. 
We support this claim with the following experiment.</p>

<h3 id="lets-try-out-our-experiment">Let’s try out our experiment</h3>

<p>We conducted an experiment to verify whether the offline GRPO method is more effective in enhancing reasoning ability than supervised fine-tuning (SFT), which only learns from positive samples. 
We used OpenThinker3-7B as our base model, and sampled approximately 2,000 Math problems from OpenThoughts3 as training data. 
The implementation was built on top of the VERL framework. 
For the training, we used a learning rate of 1e-7, a batch size of 32, 3 epochs, and a KL coefficient of 0.1. 8 samples were used per problem during training.</p>

<p>Using AIME24 as the validation set to measure performance at each epoch, the best-performing checkpoint across the three epochs was selected, and final performance was evaluated on AIME25-1 and AIME25-2. The experimental results are as follows:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>SFT</td>
      <td><strong>55.83</strong></td>
      <td>57.4</td>
    </tr>
    <tr>
      <td>Offline GRPO</td>
      <td>55.21</td>
      <td><strong>63.02</strong></td>
    </tr>
  </tbody>
</table>

<p>We show that Offline GRPO outperforms SFT by allowing the model to learn from negative samples as well.
While all training data are treated as positive in SFT, we improve the supervised learning setup by incorporating more informative reward signals through majority voting and offline GRPO.</p>

<p>In this experiment, we have demonstrated the effectiveness of Offline GRPO in further enhancing the reasoning ability of existing SOTA LLMs. We believe such method has potential to be applied to various other domains that require reasoning in the future.</p>

<h2 id="proposed-loss-for-rlvr-challenges--solutions">Proposed Loss for RLVR: Challenges &amp; Solutions</h2>
<h3 id="challenges-considering-all-positive-reasoning-trace">Challenges: Considering all positive reasoning trace</h3>
<p>A key issue arises when naively applying GRPO’s advantage calculation in the offline setting.
When all rollout trajectories are positive and the mean reward is 1, the resulting advantage becomes 0. 
This means that no updates are made for those samples.</p>

<p>In the case of online GRPO, these all-positive samples are not problematic because the model is already performing well, making further learning from such easy problems unnecessary.</p>

<p>However, in the offline setting, ignoring all-positive samples can be detrimental.
In our setup, the offline samples are sourced from a teacher model.
Even if all teacher samples are positive, this does not necessarily mean the target model can solve the same problems perfectly.
The current objective fails to capture this discrepancy, missing valuable learning opportunities from high-quality rollout trajectories.</p>

<p>The figure below shows this discrepancy in our dataset.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/plot_ot3_non_all_positive_pie.png" style="display: inline-block; width: 40%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 2. Statistics that although teacher can generate all positive answers, but base model cannot perfectly answer. </figcaption>
</figure>

<h3 id="proposed-method">Proposed method</h3>
<p>To address the issue described above, we propose a simple modification to the loss term by adding a bias term $b$ to the advantage when all rollout trajectories are positive. 
This ensures that updates are made even when all samples are positive, thus enabling learning from fully positive reasoning traces. 
Specifically, the modified loss equation is as follows:</p>

\[\mathcal{L}_{ours}(\theta) =
\begin{cases}
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t}(\hat{A}_{i,t} + b), \max(\pi_{\theta}^{i,t}, 1-\epsilon)(\hat{A}_{i,t} + b)\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{if all } r_{i} &gt; 0 \\
\\
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t} \hat{A}_{i,t}, \max(\pi_{\theta}^{i,t}, 1-\epsilon)\hat{A}_{i,t}\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{otherwise}
\end{cases}\]

<p>For our experiments, we set $b=0.1$.</p>

<h3 id="lets-try-out-our-experiment-1">Let’s try out our experiment</h3>

<p>In this experiment, we demonstrate that the proposed method improves performance over the existing offline GRPO.</p>

<p>We used the exact same experimental setup as in the first experiment, changing only the training objective.
The same model (OpenThinker3-7B), the same data (2,000 math problems from OpenThoughts3), and the same hyperparameters (learning rate=1e-7, batch size=32, epochs=3, KL coefficient=0.1, 8 samples per problem) were used. 
The bias term was set to 0.1.</p>

<p>Performance was evaluated in the same manner—selecting the best checkpoint using AIME24 as the validation set and then evaluating final performance on AIME25-1 and AIME25-2. The experimental results are as follows:</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>Offline GRPO</td>
      <td>55.21</td>
      <td>63.02</td>
    </tr>
    <tr>
      <td>Offline GRPO (with all positive group bias)</td>
      <td><strong>56.35</strong></td>
      <td><strong>63.85</strong></td>
    </tr>
  </tbody>
</table>

<p>The experimental results show that the proposed modified offline GRPO outperforms not only the base model but also the existing offline GRPO. 
We believe these improvements stem from effectively learning from cases where all teacher samples are positive, which were previously ignored.</p>

<p>In conclusion, we show that the proposed modified loss successfully addresses the limitation of existing offline GRPO in learning from all-positive samples, thereby further improving the model’s reasoning performance.
We believe this modification can also be effective in domains beyond mathematics.</p>

<h2 id="generalization-across-multiple-models-and-benchmarks">Generalization Across Multiple Models and Benchmarks</h2>

<p>We further investigated whether our proposed continual post-training methodology consistently improves performance across a broader range of models and various benchmarks.
To this end, we additionally evaluated two additional models: OpenThinker2-7B and AceReason-Nemetron-1.1-7B.
We also extended our evaluation to a broader range of benchmarks: AIME25 and AMC23 for mathematical problem-solving skills, LiveCodeBench for code generation ability, GPQA-Diamond for general reasoning ability, and IFEval for instruction-following.
We used the <a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> framework for AIME25, AMC23, LiveCodeBench, and GPQA-Diamond, and the <a href="https://github.com/EleutherAI/lm-evaluation-harness">llm-evaluation-harness</a> for IFEval.</p>

<p>The experimental results are presented in the table below:</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>AIME25</th>
      <th>AMC23</th>
      <th>LiveCodeBench</th>
      <th>GPQA-Diamond</th>
      <th>IFEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Openthinker3-7B</td>
      <td>Base</td>
      <td>57.292</td>
      <td>92.617</td>
      <td>63.968</td>
      <td>50.947</td>
      <td>50.09</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>59.532</td>
      <td>93.516</td>
      <td>65.435</td>
      <td>50.947</td>
      <td>51.14</td>
    </tr>
    <tr>
      <td>Openthinker2-7B</td>
      <td>Base</td>
      <td>39.792</td>
      <td>88.633</td>
      <td>56.115</td>
      <td>45.833</td>
      <td>53.30</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>40.573</td>
      <td>88.359</td>
      <td>56.115</td>
      <td>46.717</td>
      <td>52.62</td>
    </tr>
    <tr>
      <td>AceReason-Nemetron-1.1-7B</td>
      <td>Base</td>
      <td>64.635</td>
      <td>92.930</td>
      <td>72.383</td>
      <td>52.462</td>
      <td>36.02</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>65.573</td>
      <td>93.203</td>
      <td>71.673</td>
      <td>52.146</td>
      <td>37.38</td>
    </tr>
  </tbody>
</table>

<p>As the models were post-trained on math data, all models showed consistent performance improvements on the math-related benchmarks (AIME25, AMC23).
Interestingly, performance in other domains was maintained without degradation compared to the original models, and even showed a slight improvement on the code generation benchmark, LiveCodeBench.
These results indicate an absence of any catastrophic forgetting during the continual post-training process.</p>

<p>Additionally, offline GRPO is highly efficient as it recycles the teacher model’s trajectories without direct rollout during the training process. 
We therefore achieved performance comparable to or better than existing reinforcement learning methods while significantly reducing computational cost.
Being compute-efficient yet strong in performance, we believe that our proposed continual post-training methodology is attractive to many practitioners, and expect it to be useful across various domains and applications.
Finally, to contribute to the open-source community and ensure reproducibility, we have publicly released all the evaluated models on <a href="https://huggingface.co/collections/KRAFTON/offline-grpo-6888396558def99dd878097c">HuggingFace</a> and source code on <a href="https://github.com/krafton-ai/Offline-GRPO">GitHub</a>.</p>]]></content><author><name>KRAFTON</name></author><summary type="html"><![CDATA[In this post, we explore a new approach to enhancing the reasoning capabilities of LLMs through continual post-training. While pre-training equips LLMs with broad linguistic knowledge, it often falls short in complex reasoning tasks like math or code. Recent models have shown that Reinforcement Learning with Verifiable Rewards (RLVR) can help bridge this gap, but existing methods rely on slow and limited online training. We propose an offline alternative using teacher-generated trajectories and introduce a novel variant of Group Relative Policy Optimization (GRPO) that better captures high-quality reasoning traces—even when all outputs are positive. Our experiments on mathematical reasoning show that this method leads to consistent improvements, and all trained model checkpoints and source code have been publicly released.]]></summary></entry><entry><title type="html">[KR] Continual Post-Training of LLMs via Offline GRPO for Mathematical Reasoning</title><link href="https://krafton-ai.github.io/blog/llm_post_training_kr/" rel="alternate" type="text/html" title="[KR] Continual Post-Training of LLMs via Offline GRPO for Mathematical Reasoning" /><published>2025-07-28T00:00:00+08:00</published><updated>2025-07-28T00:00:00+08:00</updated><id>https://krafton-ai.github.io/blog/llm_post_training_kr</id><content type="html" xml:base="https://krafton-ai.github.io/blog/llm_post_training_kr/"><![CDATA[<h1 id="boosting-math-reasoning-in-open-source-llms"><strong>Boosting Math Reasoning in Open-Source LLMs</strong></h1>
<!-- Affliation: KRAFTON & SKT -->
<p>우리는 여러 추론형 Large Language Models (LLMs) 들의 수학 추론 성능을 더 끌어올리 위해 활용할 수 있는 continual post-training 방법을 제안했습니다.
기존에 많이 활용되는 on-policy GRPO는 매 step마다 model에서 rollout을 통해 sample을 뽑아야 하기 때문에 rollout 과정에서 많은 시간이 소요될 뿐 아니라, 학습 초기의 base LLM 성능이 좋지 않을 경우 높은 reward를 갖는 좋은 sample 자체를 거의 얻을 수 없다는 문제가 있습니다.
이러한 단점을 해결하기 위해, 미리 준비해둔 teacher의 reasoning trajectory만을 활용하여 offline GRPO를 진행합니다. 
추가적으로, 기존 GRPO는 group 내 모든 sample이 positive일 경우 advantage가 0이 되어 학습이 이루어지지 않는데, 여기선 teacher가 생성한 trajectory만을 다룸으로 모두 positive이더라도 supervision을 받을 수 있도록 GRPO의 advantage에 bias term을 추가하는 방식을 제안합니다.
최종적으로, Figure 1 과 같이 수학에 더 특화된 성능을 얻은 LLM들을 얻을 수 있었습니다.
또한, 이 학습 방법으로 수학이 아닌 다른 task에서는 기존 모델과 유사한 성능을 유지할 수 있음을 확인했습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/radar_charts.png" style="display: inline-block; width: 100%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 1: Performance comparison between base LLMs and our continual post-trained versions.</figcaption>
</figure>

<h2 id="why-do-we-focus-on-post-training-for-reasoning">Why do we focus on post-training for reasoning?</h2>
<p>Large text corpus로 학습하는 LLM의 사전학습(pre‑training)은 언어의 통계적 패턴과 구조를 이해하는 데 큰 역할을 합니다.
하지만 단계를 거쳐야 하는 복잡한 논리 추론, 수학적 문제 해결, 또는 코드 작성과 같은 작업에서는 충분하지 않습니다.
예컨대 Chain-of-Thought (CoT)를 쓰면 어느 정도 추론 과정을 모방하게 할 수 있지만, 이는 모델이 일관된 사고 흐름을 자기 것으로 내재화한 것이 아니라 성능의 한계가 존재합니다.</p>

<p>이런 한계를 극복하기 위한 post-training 방법 중 하나로 Reinforcement Learning with Verifiable Rewards (RLVR) 이 있습니다.
이 방식의 post-training이 LLM의 추론 강화에 효과적이라는 사실은 DeepSeek-R1 이나 OpenAI의 o1/o3 모델과 같은 모델들이 RLVR 방식을 통해 실질적인 추론 성능향상을 보이면서 더 각광을 받게 되었습니다.
RLVR은 LLM이 생성한 전체 추론 과정인 chain-of-thought(CoT) 형태의 rollout trajectory가 검증 가능해야 합니다.
예를 들어, 수학 문제의 정답이 있어서 최종 정답이 비교가 되거나, 각 단계에 대한 평가가 가능해, 생성된 rollout trajectory 전체에 대해 보상(reward)을 줄 수 있는 구조입니다.
이런 보상을 바탕으로, RL 알고리즘(e.g., PPO, GRPO)을 통해, LLM은 검증된 추론 과정을 더 많이 생성하도록 학습되게 됩니다.</p>

<p>대표적인 RLVR 방식의 학습 알고리즘으로는 Group Relative Policy Optimization (GRPO) 방식 <d-cite key="guo2025deepseekR1"></d-cite>이 있습니다.
이 방식은 동일 질문에 대해 샘플링한 여러 rollout trajectory들의 그룹의 평균 보상을 기준으로 삼아 각 응답의 상대적 advantage를 계산합니다.
이의 objective $\mathcal{L}$을 수식으로 표현하면 아래와 같습니다:</p>

\[\mathcal{L}_{GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{old}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{old}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>여기서, $\pi^{i,t} = \pi(o_{i,t}\mid q, o_{i,&lt;t})$이며, $G$는 한 그룹 내 응답 개수를 의미합니다. 또한, $\pi_\text{ref}$, $\pi_{\theta}$, $\pi_\text{old}$는 각각 기준(reference) LLM, 학습 중인 LLM, 샘플링에 사용되는 LLM을 나타냅니다. $q$와 ${o_i}_{i=1}^{G}$는 학습에 사용되는 질문과 생성된 rollout trajectory 세트를 나타냅니다.
각 응답의 상대적 advantage인 $\hat{A}_i$는 다음과 같이 계산됩니다.</p>

\[\hat{A}_i = \frac{r_i - \text{mean}(r_i)}{\text{std}(r_i)}\]

<p>이렇게 계산된 trajectory level 의 advantage는 응답의 각 토큰 수준에 적용되어 최종적으로 $\hat{A}_{i,t}$로 사용됩니다.</p>

<h2 id="offline-reinforcement-learning-with-verifiable-reward">Offline Reinforcement Learning with Verifiable Reward</h2>
<h3 id="why-do-we-focus-on-offline-rlvr-eg-grpo">Why do we focus on Offline RLVR (e.g., GRPO)?</h3>
<p>앞서 설명한 RLVR 의 경우는 on-policy 로 advantage를 업데이트 하는 방식입니다.
On-policy 의 경우, 학습하는 모델이 뽑아낸 roll-out trajectory 에 대해 reward 를 계산해 이를 통해 LLM을 업데이트 한다는 것을 의미합니다.</p>

<p>하지만, on-policy로 학습하는 것에는 두가지 단점이 존재합니다.
첫번째 단점은 학습 속도가 매우 느리다는 것입니다. <d-cite key="liang2025squeeze"></d-cite>
이 이유는 매 스텝 loss 를 계산할 때 roll-out 을 각 문제마다 $G$ 개수만큼 해야하기 때문입니다.
또한, on-policy 는 기존 base LLM 성능에 한계가 제한됩니다. <d-cite key="luffy"></d-cite>
기존 base LLM 의 성능이 낮으면 정답 trajectory 를 뽑을 확률이 낮아져 성능 향상의 bottleneck 이 생길 수 있기 때문입니다.</p>

<p>On-policy 의 문제들을 해결하기 위해, 저희는 정답 label 을 뽑기 위해 쓴 teacher LLM의 roll-out trajectory 를 활용하는 offline RLVR 방식을 도입했습니다.
저희가 이번 프로젝트에 활용한 데이터는 OpenThought3 <d-cite key="openthoughts"></d-cite> 데이터셋입니다.
OpenThought3는 Math, Code, Science 등에 관련된 문제들에 대해 teacher model 인 QwQ-32B 모델을 이용해 sampling 을 16번씩 진행해 만든 데이터입니다.
이 중, 저희는 정답이 확실한 Math 에 대해서만 먼저 filtering 하여 준비했습니다.
그 뒤, 각 문제마다 Majority Voting <d-cite key="wangself"></d-cite> 을 이용해 정답으로 간주될 수 있는 답들을 찾고, 이를 바탕으로 각 trajectory 의 reward 를 계산하였습니다.</p>

<p>이 때, Teacher model 을 $\theta_\text{teacher}$ 라고 하고, teacher가 정답 label을 뽑기 위해 roll-out 한 문제들의 random variable을 $Q$ 라고 할 때, 우리는 offline GRPO를 아래와 같이 수식을 쓸 수 있습니다:</p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}}\hat{A}_{i,t},\text{clip}\left(\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}},1-\epsilon,1+\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<p>이 때, practical 하게 teacher 의 probability 를 모르기 때문에, $\pi_{\theta_\text{teacher}}^{i,t}=1$ 로 가정하였습니다.
그러면 항상 $\frac{\pi_{\theta}^{i,t}}{\pi_{\theta_\text{teacher}}^{i,t}} \le 1$ 이기 때문에, $\text{clip}$ 은 $\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)$ 로 표현될 수 있습니다.
최종적으로, practical하게 offline 에 해당하는 GRPO 수식은 아래와 같이 근사될 수 있습니다. <d-cite key="luffy"></d-cite></p>

\[\mathcal{L}_{Off-GRPO}(\theta) = \mathbb{E}_{[q \sim P(Q), \{o_i\}_{i=1}^{G}\sim \pi_{\theta_\text{teacher}}(O|q)]}\left[
\frac{1}{G}\sum_{i=1}^{G}\frac{1}{|o_i|}\sum_{t=1}^{|o_i|}\left\{
\min\left[\pi_{\theta}^{i,t}\hat{A}_{i,t},\max\left(\pi_{\theta}^{i,t},1-\epsilon\right)\hat{A}_{i,t}\right]
-\beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right\}
\right]\]

<h3 id="offline-grpo-vs-supervised-fine-tuning-sft">Offline GRPO vs. Supervised Fine-tuning (SFT)</h3>

<p>제일 간단하게 offline으로 준비된 teacher sample 로 학습한다는 것은 SFT를 하는 것으로 생각할 수 있습니다.
그러면 SFT 와 Offline GRPO 는 무슨 차이가 있을까요?
아래의 표현된 SFT 수식과 offline GRPO 의 수식을 비교하면 그 차이를 알 수 있습니다.</p>

\[\mathcal{L}_{SFT}(\theta) = \mathbb{E}_{q \sim P(Q),\,o \sim \pi_{\theta_\text{teacher}}^{(+)}(O|q)}\left[\frac{1}{|o|}\sum_{t=1}^{|o|}\log\pi_{\theta}(o_{t}|q,o_{\lt t}) - \beta\mathbb{D}_{KL}\left[\pi_{\theta}\mid\mid\pi_\text{ref}\right]\right]\]

<p>여기서, $\pi_{\theta_\text{teacher}}^{(+)}(O|q)$는 teacher가 생성한 샘플 중에서 positive reward를 갖는 샘플만 선택했음을 나타냅니다.
이를 통해 모델은 오로지 positive 샘플의 분포를 따라가도록 지도학습을 수행하게 됩니다.</p>

<p>Practical하게 근사한 offline GRPO 수식과 비교했을 때, 우리의 Offline GRPO 수식은 negative 에 해당하는 sample 또한 반영된다는 점입니다.
Offline GRPO 에서는 positive 와는 계속 가까워지게 학습을 하며, 반대로 negative 와는 멀어지게 학습을 하는 구조가 됩니다.
이로 인해 논리적으로 틀려 가지 않아야 할 rollout trajectory 에 멀어지게 되어, 더 추론 능력이 강화될 수 있습니다.
이 사실을 아래 실험을 통해 확인해 보았습니다.</p>

<h3 id="lets-try-out-our-experiment">Let’s try out our experiment</h3>

<p>우리는 offline GRPO 방식이 단순히 positive 샘플만을 학습하는 supervised fine-tuning (SFT) 방식보다 추론 능력 강화에 더 효과적인지 확인하기 위해 실험을 수행했습니다. 
실험에 사용된 모델은 OpenThinker3-7B이며, 데이터는 OpenThought3의 Math 문제 중 약 2,000개를 sampling하여 구성했습니다. 
실험 환경은 VERL 프레임워크를 기반으로 구축되었고, 학습 과정에서 learning rate는 1e-7, batch size는 32, epoch는 3, KL coefficient는 0.1로 설정했습니다. 이때 각 문제당 8개의 샘플을 사용하여 학습을 진행했습니다.</p>

<p>성능 평가는 AIME24를 validation set으로 사용하여 각 epoch마다 성능을 측정했고, 3 epoch 중 가장 우수한 성능을 보인 checkpoint를 선정하여 최종적으로 AIME25-1과 AIME25-2에서 성능을 비교했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>SFT</td>
      <td><strong>55.83</strong></td>
      <td>57.40</td>
    </tr>
    <tr>
      <td>Offline GRPO</td>
      <td>55.21</td>
      <td><strong>63.02</strong></td>
    </tr>
  </tbody>
</table>

<p>Offline GRPO 방식이 SFT보다 종합적으로 더 뛰어난 성능을 보인 이유는 negative sample들로부터 모델이 적극적으로 멀어지도록 학습을 유도했기 때문입니다. 
기존의 데이터는 모두 positive sample로 고려되고 있지만, 본 실험에서는 majority voting과 Offline GRPO를 통해 보다 정확한 정보를 확보하고 이를 바탕으로 negative sample로부터 벗어나도록 유도하여, 더욱 세밀한 지도학습이 가능했습니다.</p>

<p>본 실험을 통해 Offline GRPO 방식이 기존 SOTA LLM의 논리적 추론 능력을 한 단계 더 향상시키는 데 매우 효과적임을 입증했습니다. 이 방식은 향후 다양한 추론 도메인에서도 충분히 적용 가능할 것으로 기대됩니다.</p>

<h2 id="proposed-loss-for-rlvr-challenges--solutions">Proposed Loss for RLVR: Challenges &amp; Solutions</h2>
<h3 id="challenges-considering-all-positive-reasoning-trace">Challenges: Considering all positive reasoning trace</h3>
<p>Offline GRPO 는 그럼 문제가 없을까요?
아닙니다. 이유는 roll-out trajectory 가 all positive인 sample의 Advantage 값에 있습니다.
실제로 어떤 quesiton에 대해서는 전부 다 positive 인 sample 이 존재 할 수 있습니다.
이런 데이터의 경우 advantage 값을 계산하게 되면 mean reward 가 1 이 되어 0이 되고, 그러면 그 sample 들에 대해서는 update 가 되지 않습니다.</p>

<p>On-policy 의 경우에서는 이런 all positive sample 들이 부분이 문제가 안됩니다.
왜냐하면 이미 모델이 잘하는 경우라 update 를 하지 않아도 되기 때문입니다.</p>

<p>하지만, 반대로 Offline 의 경우에는 all positive sample 들을 학습하지 않는 것이 도움이 되지 않는 경우가 있습니다.
Offline으로 준비한 sample 이 teacher sample 인 저희의 case 의 경우를 생각해 보겠습니다.
이 경우, teacher sample 은 전부 다 positive 여서 reward 가 1이지만, 실제 그 문제에 대해 저희가 학습할 target 모델은 틀린 sample 을 생성할 가능성이 높을 수 있습니다.
하지만 현재 Objective 로는 이 부분을 고려하지 못하고, 결국 배워야 할 rollout trajectory 에 대해 학습하지 못함을 의미합니다.</p>

<p>저희가 준비한 데이터에서 이 문제가 있음을 아래 그림을 통해 확인이 되었습니다.</p>

<figure style="text-align: center;">
  <img src="/blog/assets/img/2025-07-28-llm_post_training/plot_ot3_non_all_positive_pie.png" style="display: inline-block; width: 40%; height: auto;" />
  <figcaption style="font-size: 1em;">Figure 2. Statistics that although teacher can generate all positive answers, but base model cannot perfectly answer. </figcaption>
</figure>

<h3 id="proposed-method">Proposed method</h3>
<p>위 문제를 해결하기 위해, 저희는 all positive roll-out trajectory 인 경우에는 advantage 에 bias term 인 $b$를 추가하는 simple 한 변형 loss term 을 제안하였습니다.
제안 방식을 통해 all positive 인 sample 이 나올 때도 그 sample 들에 대해 update 가 되도록 해주어서, all positive reasoning trace를 반영하지 못하는 문제를 해결합니다.
자세한 수식은 아래와 같습니다.</p>

\[\mathcal{L}_{ours}(\theta) =
\begin{cases}
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t}(\hat{A}_{i,t} + b), \max(\pi_{\theta}^{i,t}, 1-\epsilon)(\hat{A}_{i,t} + b)\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{if all } r_{i} &gt; 0 \\
\\
\mathbb{E}\left[
\frac{1}{G} \sum\limits_{i=1}^{G} \frac{1}{|o_i|} \sum\limits_{t=1}^{|o_i|}
\left\{
\min\left[\pi_{\theta}^{i,t} \hat{A}_{i,t}, \max(\pi_{\theta}^{i,t}, 1-\epsilon)\hat{A}_{i,t}\right]
- \beta \mathbb{D}_{KL}\left[\pi_{\theta} \,\|\, \pi_\text{ref} \right]
\right\}
\right] &amp; \text{otherwise}
\end{cases}\]

<p>저희는 $b=0.1$ 로 setting 을 하고 실험을 진행했습니다.</p>

<h3 id="lets-try-out-our-experiment-1">Let’s try out our experiment</h3>

<p>본 실험에서는 제안한 방식이 기존 offline GRPO보다 성능 향상에 도움이 되는지 추가로 검증하고자 하였습니다.</p>

<p>실험 환경은 첫 번째 실험과 완벽히 동일하며, 오직 학습 objective만 달라졌습니다. 
동일한 모델(OpenThinker3-7B), 동일한 데이터(OpenThought3 Math 문제 2,000개), 동일한 hyperparameter(learning rate=1e-7, batch size=32, epoch=3, KL coefficient=0.1, 각 문제당 8개의 샘플)를 사용했습니다. 
또한 bias term은 0.1로 설정하여 진행했습니다.</p>

<p>성능 평가는 동일한 방식으로 AIME24를 validation set으로 사용하여 best checkpoint를 선정한 후, 최종적으로 AIME25-1과 AIME25-2를 통해 성능을 평가했습니다. 실험 결과는 다음과 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>AIME25-1 Score</th>
      <th>AIME25-2 Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Base model</td>
      <td>55.31</td>
      <td>61.25</td>
    </tr>
    <tr>
      <td>Offline GRPO</td>
      <td>55.21</td>
      <td>63.02</td>
    </tr>
    <tr>
      <td>Offline GRPO (with all positive group bias)</td>
      <td><strong>56.35</strong></td>
      <td><strong>63.85</strong></td>
    </tr>
  </tbody>
</table>

<p>실험 결과, 제안한 modified offline GRPO 방식이 base model 뿐만 아니라 기존 offline GRPO 방식보다도 더 높은 성능을 기록했습니다. 
이는 앞서 언급된 바와 같이, 기존 offline GRPO 방식에 더해 all positive sample 들까지 학습했기 때문으로 추측됩니다.</p>

<p>종합적으로, 제안한 modified loss 방식이 기존 offline GRPO의 한계였던 all-positive 샘플의 학습 문제를 성공적으로 극복했으며, 이를 통해 추론 성능을 더욱 강화할 수 있음을 확인했습니다. 
향후 다른 다양한 추론 도메인에서도 이 방법론이 효과적으로 활용될 가능성이 클 것으로 기대됩니다.</p>

<h2 id="generalization-across-multiple-models-and-benchmarks">Generalization Across Multiple Models and Benchmarks</h2>

<p>우리는 제안한 continual post-training 방법론이 더 광범위한 모델과 다양한 벤치마크에서도 일관된 성능 개선을 보이는지 추가적으로 확인해 보았습니다. 
이를 위해, 추가적으로 두 가지 모델(Openthinker2-7B, AceReason-Nemetron-1.1-7B)을 사용해 평가했습니다. 
또한 평가를 위해 선택한 벤치마크는 수학 문제 해결 능력을 평가하는 AIME25와 AMC23, 코드 생성 능력을 평가하는 LiveCodeBench, 일반적 추론 능력을 평가하는 GPQA-Diamond, 그리고 사실 검증 성능을 평가하는 IFEval입니다.
여기서 AIME25와 AMC23, LiveCodeBench, GPQA-Diamond 평가에는 <a href="https://github.com/NovaSky-AI/SkyThought">SkyThought</a> framework, IFEval 평가엔 <a href="https://github.com/EleutherAI/lm-evaluation-harness">llm-evaluation-harness</a> framework을 사용하였습니다.</p>

<p>실험 결과는 아래 표와 같습니다.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Method</th>
      <th>AIME25</th>
      <th>AMC23</th>
      <th>LiveCodeBench</th>
      <th>GPQA-Diamond</th>
      <th>IFEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Openthinker3-7B</td>
      <td>Base</td>
      <td>57.292</td>
      <td>92.617</td>
      <td>63.968</td>
      <td>50.947</td>
      <td>50.09</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>59.532</td>
      <td>93.516</td>
      <td>65.435</td>
      <td>50.947</td>
      <td>51.14</td>
    </tr>
    <tr>
      <td>Openthinker2-7B</td>
      <td>Base</td>
      <td>39.792</td>
      <td>88.633</td>
      <td>56.115</td>
      <td>45.833</td>
      <td>53.30</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>40.573</td>
      <td>88.359</td>
      <td>56.115</td>
      <td>46.717</td>
      <td>52.62</td>
    </tr>
    <tr>
      <td>AceReason-Nemetron-1.1-7B</td>
      <td>Base</td>
      <td>64.635</td>
      <td>92.930</td>
      <td>72.383</td>
      <td>52.462</td>
      <td>36.02</td>
    </tr>
    <tr>
      <td> </td>
      <td>Offline GRPO (+bias)</td>
      <td>65.573</td>
      <td>93.203</td>
      <td>71.673</td>
      <td>52.146</td>
      <td>37.38</td>
    </tr>
  </tbody>
</table>

<p>실험 결과를 통해 Math 데이터로 post-training한 만큼 수학 관련 벤치마크(AIME25, AMC23)에서는 모든 모델에서 일관된 성능 향상이 관찰되었습니다. 
흥미롭게도, 다른 도메인에서의 성능 또한 기존 대비 저하되지 않고 유지되었으며, 특히 코드 생성 벤치마크인 LiveCodeBench에서는 오히려 소폭의 성능 향상이 관찰되었습니다. 
이는 continual post-training 과정에서도 catastrophic forgetting이 거의 발생하지 않았다는 것을 의미합니다.</p>

<p>추가적으로, offline GRPO 방식은 실제 학습 과정에서 직접적인 rollout 없이 teacher model의 trajectory를 재활용하기 때문에 매우 효율적입니다. 
이러한 효율성 덕분에 기존의 reinforcement learning 방식 대비 계산 자원을 현저히 절약하면서도, 성능은 동등하거나 더 뛰어난 결과를 달성했습니다. 
따라서 저희가 제안한 continual post-training 방법론은 성능과 효율성을 모두 갖춘 실용적인 접근법으로, 향후 다양한 도메인과 응용 분야에서도 매우 유용하게 활용될 수 있을 것으로 기대합니다.
마지막으로, 오픈 소스 커뮤니티에 기여하고 연구의 재현성과 확장 가능성을 높이기 위해 <a href="https://huggingface.co/collections/KRAFTON/offline-grpo-6888396558def99dd878097c">HuggingFace</a>와 <a href="https://github.com/krafton-ai/Offline-GRPO">GitHub</a>에 평가한 모델들과 학습 코드 모두를 공개하였습니다.</p>]]></content><author><name>KRAFTON</name></author><summary type="html"><![CDATA[이 포스팅에서는 거대 언어 모델(LLM)의 추론 능력을 지속적인 추가 학습(Continual Post-training)을 통해 향상시키는 새로운 접근법을 소개합니다. 기존의 사전 학습(Pre-training)은 광범위한 언어적 지식을 제공하지만, 수학이나 코드 생성과 같은 복잡한 추론 작업에서는 여전히 한계를 드러냅니다. 최근 연구에서는 강화학습 기반의 검증 가능한 보상 기법(RLVR)이 이러한 한계를 어느 정도 해소할 수 있다고 밝혔으나, 현재까지 제안된 방법들은 느리고 제한적인 온라인 (Online) 학습에 의존하고 있습니다. 우리는 이 문제를 해결하기 위해 교사 모델이 생성한 우수한 추론 경로(Trajectory)만을 활용하는 오프라인 (Offline) 학습 방식을 제안하며, 특히 모든 생성된 결과가 긍정적인 경우에도 고품질의 추론 과정을 효과적으로 학습할 수 있도록 GRPO 알고리즘의 새로운 변형 방식을 도입했습니다. 수학적 추론을 대상으로 한 실험 결과, 제안한 방법이 일관된 성능 향상을 보임을 확인하였으며, 학습된 모든 모델의 checkpoint와 학습 코드를 공개하였습니다.]]></summary></entry></feed>